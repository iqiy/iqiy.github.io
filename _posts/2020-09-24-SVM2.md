---
layout:     post
title:      "ML学习笔记（一.一）"
subtitle:   "SVM基础2"
date:       2020-09-24 20:33:18
author:     "QIY"
header-img: "img/ML.png"
header-mask: 0.3 
catalog:    true
tags:
    - Machine Learning
---


> Machine Learning 的学习需稳扎稳打

* TOC
{:toc}



# 2.优化方法

=
0$$表示，只有距离最优超平面的支持向量(xi,yi)对应的α非零，其他所有点集的α等于零。综上所述，引入拉格朗日乘子以后，我们的目标变为：

$$
\min_{w,b}\max_{\alpha \geqslant 0}L\left( w,b,\alpha \right)
$$

即先求得α的极大值，再求w和b的极小值。

对偶法：

$$
\max_{\alpha \geqslant 0}\min_{w,b}L(w,b,\alpha)
$$

即先求得w和b的极小值，在求α的极大值。用L(w,b,α)对ww和b分别求偏导，并令其等于0：

$$
\{\begin{matrix}
 & \frac{\partial L(w,b,\alpha)}{\partial w} = 0 \\
 & \\
 & \frac{\partial L(w,b,\alpha)}{\partial b} = 0 \\
\end{matrix}
$$

得：

$$
\{\begin{matrix}
 & w = \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}}x_{{}_{i}} \\
 & \\
 & \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}} = 0 \\
\end{matrix}
$$

把该式代入原来的的拉格朗日式子可得（推导过程省略）：

$$
W\left( \alpha \right) = \sum_{i = 1}^{n}{}\alpha{}_{i} - \frac{1}{2}\sum_{i = 1}^{n}{}\sum_{j = 1}^{n}{}\alpha{}_{i}\alpha{}_{j}y_{{}_{i}}y_{{}_{j}}{x_{{}_{i}}}^{T}x_{{}_{j}}
$$

$$
\sum_{i = 1}^{n}{}\alpha{}_{i}y_{{}_{i}} = 0,\alpha_{{}_{i}} \geqslant 0(i = 1,...,n)
$$

通过SMO方法求a，后求w，b。

$$
w^{*} = \sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}x_{{}_{i}}
$$

$$
b^{*} = 1 - {w^{*}}^{T}x_{{}_{s}}
$$

# 3 非线性可分

现实世界的许多问题并不都是线性可分的，尤其存在许多复杂的非线性可分的情形。

![](/img/in-post/200924_svm2/326fdcb0715860f8aca9c9bb1774c5aa.jpg)

要解决这些不可分问题，一般有两种方法。第一种是放宽过于严格的间隔，构造软间隔。另一种是运用核函数把这些数据映射到另一个维度空间去解决非线性问题。

## 3.1 软间隔

$$
\{\begin{matrix}
 & \min_{w,b}\frac{1}{2}||w||^{2} + C\sum_{i = 1}^{n}\xi_{{}_{i}} & (\text{hinge} - \text{loss}) & \\
 & & & \\
 & y_{{}_{i}}\left( w^{T}x_{{}_{i}} + b \right) \geqslant 1 - \xi_{{}_{i}} & ,\xi_{{}_{i}} \geqslant 0, & \left( i = 1,..,n \right) \\
\end{matrix}
$$

同样拉格朗日，对偶，kkt条件得：

![](/img/in-post/200924_svm2/c8c09979e58459d90c78c097209b77c2.png)

可以看到，松驰变量ξi没有出现在W(α)中，线性可分与不可分的差异体现在约束αi⩾0被替换成了约束0⩽αi⩽C。但是，这两种情况下求解ww和bb是非常相似的，对于支持向量的定义也都是一致的。  
在不可分情况下，对应的KKT条件为：

$$
\alpha_{{}_{i}}\lbrack y_{{}_{i}}(w^{T}x_{{}_{i}} + b) - 1 + \xi_{{}_{i}}\rbrack = 0,(i = 1,...,n)
$$

## 3.2 核方法

核函数能够恰当的计算给定数据的内积，将数据从输入空间的非线性转变到特征空间，特征空间具有更高甚至无限的维度，从而使得数据在该空间中被转换成线性可分的。如下图所示，我们把二维平面的一组数据，通过核函数映射到了一个三维空间中，这样，我们的超平面就面成了一个平面（在二维空间中是一条直线），这个平面就可以准确的把数据划分开了。

![](/img/in-post/200924_svm2/e9d28967b89baf3f5f38b27c54bcdb10.jpg)

![](/img/in-post/200924_svm2/c669cf8cbfa024d65151cd43fba578a0.jpg)

![](/img/in-post/200924_svm2/27f457c32841ae33edacab1d81692bcd.jpg)

![](/img/in-post/200924_svm2/1e54d00ad9165591182151863a4d5a0f.jpg)

核函数有Sigmoid核、线性核、多项式核和高斯核等，其中高斯核和多项式核比较常用，两种核函数均可以把低维数据映射到高维数据。高斯核的公式如下，σ是达到率，即函数值跌落到0的速度参数：

$$
K\left( x_{1},x_{2} \right) = exp\left( \frac{- ||x_{1} - x_{2}||^{2}}{2\sigma^{2}} \right)
$$

多项式核函数的公式如下，R为实数，d为低维空间的维数：

$$
K(x_{1},x_{2}) = (\langle x_{1},x_{2}\rangle + R)^{d}
$$

应用于我们的上个例子，我们先定义，用ϕ:x→H表示从输入空间x⊂Rn到特征空间H的一个非线性变换。假设在特征空间中的问题是线性可分的，那么对应的最优超平面为：

$$
w^{\text{ϕT}}\phi\left( x \right) + b = 0
$$

通过拉格朗日函数我们推导出:

$$
w^{\phi*} = \sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}\phi\left( x_{{}_{i}} \right)
$$

带入上式得特征空间的最优超平面为：

$$
\sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}\phi^{T}(x_{{}_{i}})\phi(x) + b = 0
$$

这里的ϕT(xi)ϕ(x)表示内积，用核函数代替内积则为：

![](/img/in-post/200924_svm2/ac449b5a87680f85a5659b2ef740d56d.png)

## 3.3 核函数解释

假设有两个输入样本，它们均为二维行向量x1=[x1,x2]x1=[x1,x2]他们的内积为：

𝐾(𝑥1,𝑥2)=(⟨𝑥1,𝑥2⟩)2=(𝑥1𝑥3+𝑥2𝑥4)2=𝑥12𝑥32+2𝑥1𝑥2𝑥3𝑥4+𝑥22𝑥42=𝜙(𝑥1)𝜙(𝑥2)

![](/img/in-post/200924_svm2/75567ca49f9cf2978be7c1ee39e74efc.png)

这样我们就把二维数据映射成了三维数据，对于高斯核的映射，会用到泰勒级数展开式，读者可以自行推导一下。

#4 SMO算法

SMO算法的目标是求出一系列α，一旦求出了这些α，就很容易计算出权重向量w和b，并得到分隔超平面。  
SMO算法的工作原理是：每次循环中选择两个α进行优化处理。一旦找到一对合适的α，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个α必须要符合一定的条件，条件之一就是这两个α必须要在间隔边界之外，而其第二个条件则是这两个α还没有进行过区间化处理或者不在边界上。
