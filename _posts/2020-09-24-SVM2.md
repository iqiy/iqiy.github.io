---
layout:     post
title:      "ML学习笔记（一.一）"
subtitle:   "SVM基础2"
date:       2020-09-24 20:33:18
author:     "QIY"
header-img: "img/ML.png"
header-mask: 0.3 
catalog:    true
tags:
    - Machine Learning
---


> Machine Learning 的学习需稳扎稳打

* TOC
{:toc}


# 引言

-   什么是SVM？SVM干嘛用？

![](/img/in-post/200924_svm2/20651249bdc017632b540567c2f39f15.png)

SVM是什么:（Support Vectors）支持向量机；

SVM干嘛: 通过寻找最优超平面**函数**，对数据**二分类**。

支持向量（Support
Vectors）：划分数据并且两平面之间没有数据点，该两平面（后续记H1/H2）上的**数据点集**。

最优超平面：H1/H2间距离最大化，中间平面函数。

本章主要内容：

![](/img/in-post/200924_svm2/907ffb633624c2d405f68de0d997cef7.png)

# 1几何推导

-   目的：寻找最大间隔就是寻找最优超平面

-   思路（线性可分情况）

    -   1）读取你的数据集。

    -   2）找到两个平行超平面，可以划分数据并且**两平面之间没有数据点**。

    -   3）最大化上述两个超平面间隔

-   结果：**约束条件为,𝑦𝑖(𝐰,𝐱-.−𝑏)≥1（对于𝑖=1,…,𝑛），的情况下找到‖w‖最小的w,𝑏，**
    不等式约束的最小化问题。

## 1.1 **步骤1**

**读取数据集**

![](/img/in-post/200924_svm2/a07eb337dd0cec2fd4285583f8ded161.png)

数据D集 是n个元素 对组成的集合。yi代表元素属于类(-1)或类(+1)。

## 1.2 **步骤2**

**找到两个平行超平面，可以划分数据并 且两平面之间没有数据点**。

![](/img/in-post/200924_svm2/87ce89ee13f743cd7fa00b763fa23de7.png)

### 超平面H0划分数据集并且满足：

$$
\mathbf{w}\mathbf{\cdot}\mathbf{x}\mathbf{- b = 0}
$$

H1：

$$
\mathbf{w} \cdot \mathbf{x} - b = \delta
$$

H2：

$$
\mathbf{w} \cdot \mathbf{x} - b = - \delta
$$

这里$$\mathbf{w}\mathbf{,}b,\delta$$等比例，我们索性让$$\delta$$等于1，它**们之间没有任何的数据点**。我们只选择那些符合以下两个约束的超平面：

![](/img/in-post/200924_svm2/2c8b91d616afb961ab203a3584aa06b4.png)

![](/img/in-post/200924_svm2/4832987e501a9929c0dedd41de083416.png)

可化简：𝑦(𝐰⋅𝐱𝐢−𝑏)≥1for all1≤𝑖≤𝑛。

## 1.3 **步骤3**

# 2.优化方法

$$
\{\begin{matrix}
 & \max_{w,b}\frac{2}{||w||} \\
 & \\
 & y_{{}_{i}}(w^{T}x_{{}_{i}} + b) \geqslant 1,(i = 1,..,n) \\
\end{matrix}
$$

这种式子通常我们用拉格朗日乘数法来求解，即：

$$
L(x) = f(x) + \sum\alpha g(x)
$$

$$f(x)$$是我们需要最小化的目标函数，$$g(x)$$是不等式约束条件，即前面的$$y_{{}_{i}}(w^{T}x_{{}_{i}}
+ b) \geqslant 1$$，  
α是对应的约束系数，也叫拉格朗日乘子。为了使得拉格朗日函数得到最优化解，我们需要加入能使该函数有最优化解法的KKT条件，或者叫最优化条件、充要条件。

$$
L(w,b,\alpha) = \frac{1}{2}w^{T}w - \sum_{i = 1}^{n}{}\alpha_{{}_{i}}\lbrack y_{{}_{i}}(w^{T}x_{{}_{i}} + b) - 1\rbrack
$$

以上的KKT条件$$\alpha_{{}_{i}}\lbrack y_{{}_{i}}(w^{T}x_{{}_{i}} + b) - 1\rbrack
=
0$$表示，只有距离最优超平面的支持向量(xi,yi)对应的α非零，其他所有点集的α等于零。综上所述，引入拉格朗日乘子以后，我们的目标变为：

$$
\min_{w,b}\max_{\alpha \geqslant 0}L\left( w,b,\alpha \right)
$$

即先求得α的极大值，再求w和b的极小值。

对偶法：

$$
\max_{\alpha \geqslant 0}\min_{w,b}L(w,b,\alpha)
$$

即先求得w和b的极小值，在求α的极大值。用L(w,b,α)对ww和b分别求偏导，并令其等于0：

$$
\{\begin{matrix}
 & \frac{\partial L(w,b,\alpha)}{\partial w} = 0 \\
 & \\
 & \frac{\partial L(w,b,\alpha)}{\partial b} = 0 \\
\end{matrix}
$$

得：

$$
\{\begin{matrix}
 & w = \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}}x_{{}_{i}} \\
 & \\
 & \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}} = 0 \\
\end{matrix}
$$

把该式代入原来的的拉格朗日式子可得（推导过程省略）：

$$
W\left( \alpha \right) = \sum_{i = 1}^{n}{}\alpha{}_{i} - \frac{1}{2}\sum_{i = 1}^{n}{}\sum_{j = 1}^{n}{}\alpha{}_{i}\alpha{}_{j}y_{{}_{i}}y_{{}_{j}}{x_{{}_{i}}}^{T}x_{{}_{j}}
$$

$$
\sum_{i = 1}^{n}{}\alpha{}_{i}y_{{}_{i}} = 0,\alpha_{{}_{i}} \geqslant 0(i = 1,...,n)
$$

通过SMO方法求a，后求w，b。

$$
w^{*} = \sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}x_{{}_{i}}
$$

$$
b^{*} = 1 - {w^{*}}^{T}x_{{}_{s}}
$$

# 3 非线性可分

现实世界的许多问题并不都是线性可分的，尤其存在许多复杂的非线性可分的情形。

![](/img/in-post/200924_svm2/326fdcb0715860f8aca9c9bb1774c5aa.jpg)

要解决这些不可分问题，一般有两种方法。第一种是放宽过于严格的间隔，构造软间隔。另一种是运用核函数把这些数据映射到另一个维度空间去解决非线性问题。

## 3.1 软间隔

$$
\{\begin{matrix}
 & \min_{w,b}\frac{1}{2}||w||^{2} + C\sum_{i = 1}^{n}\xi_{{}_{i}} & (\text{hinge} - \text{loss}) & \\
 & & & \\
 & y_{{}_{i}}\left( w^{T}x_{{}_{i}} + b \right) \geqslant 1 - \xi_{{}_{i}} & ,\xi_{{}_{i}} \geqslant 0, & \left( i = 1,..,n \right) \\
\end{matrix}
$$

同样拉格朗日，对偶，kkt条件得：

![](/img/in-post/200924_svm2/c8c09979e58459d90c78c097209b77c2.png)

可以看到，松驰变量ξi没有出现在W(α)中，线性可分与不可分的差异体现在约束αi⩾0被替换成了约束0⩽αi⩽C。但是，这两种情况下求解ww和bb是非常相似的，对于支持向量的定义也都是一致的。  
在不可分情况下，对应的KKT条件为：

$$
\alpha_{{}_{i}}\lbrack y_{{}_{i}}(w^{T}x_{{}_{i}} + b) - 1 + \xi_{{}_{i}}\rbrack = 0,(i = 1,...,n)
$$

## 3.2 核方法

核函数能够恰当的计算给定数据的内积，将数据从输入空间的非线性转变到特征空间，特征空间具有更高甚至无限的维度，从而使得数据在该空间中被转换成线性可分的。如下图所示，我们把二维平面的一组数据，通过核函数映射到了一个三维空间中，这样，我们的超平面就面成了一个平面（在二维空间中是一条直线），这个平面就可以准确的把数据划分开了。

![](/img/in-post/200924_svm2/e9d28967b89baf3f5f38b27c54bcdb10.jpg)

![](/img/in-post/200924_svm2/c669cf8cbfa024d65151cd43fba578a0.jpg)

![](/img/in-post/200924_svm2/27f457c32841ae33edacab1d81692bcd.jpg)

![](/img/in-post/200924_svm2/1e54d00ad9165591182151863a4d5a0f.jpg)

核函数有Sigmoid核、线性核、多项式核和高斯核等，其中高斯核和多项式核比较常用，两种核函数均可以把低维数据映射到高维数据。高斯核的公式如下，σ是达到率，即函数值跌落到0的速度参数：

$$
K\left( x_{1},x_{2} \right) = exp\left( \frac{- ||x_{1} - x_{2}||^{2}}{2\sigma^{2}} \right)
$$

多项式核函数的公式如下，R为实数，d为低维空间的维数：

$$
K(x_{1},x_{2}) = (\langle x_{1},x_{2}\rangle + R)^{d}
$$

应用于我们的上个例子，我们先定义，用ϕ:x→H表示从输入空间x⊂Rn到特征空间H的一个非线性变换。假设在特征空间中的问题是线性可分的，那么对应的最优超平面为：

$$
w^{\text{ϕT}}\phi\left( x \right) + b = 0
$$

通过拉格朗日函数我们推导出:

$$
w^{\phi*} = \sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}\phi\left( x_{{}_{i}} \right)
$$

带入上式得特征空间的最优超平面为：

$$
\sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}\phi^{T}(x_{{}_{i}})\phi(x) + b = 0
$$

这里的ϕT(xi)ϕ(x)表示内积，用核函数代替内积则为：

![](/img/in-post/200924_svm2/ac449b5a87680f85a5659b2ef740d56d.png)

## 3.3 核函数解释

假设有两个输入样本，它们均为二维行向量x1=[x1,x2]x1=[x1,x2]他们的内积为：

𝐾(𝑥1,𝑥2)=(⟨𝑥1,𝑥2⟩)2=(𝑥1𝑥3+𝑥2𝑥4)2=𝑥12𝑥32+2𝑥1𝑥2𝑥3𝑥4+𝑥22𝑥42=𝜙(𝑥1)𝜙(𝑥2)

![](/img/in-post/200924_svm2/75567ca49f9cf2978be7c1ee39e74efc.png)

这样我们就把二维数据映射成了三维数据，对于高斯核的映射，会用到泰勒级数展开式，读者可以自行推导一下。

#4 SMO算法

SMO算法的目标是求出一系列α，一旦求出了这些α，就很容易计算出权重向量w和b，并得到分隔超平面。  
SMO算法的工作原理是：每次循环中选择两个α进行优化处理。一旦找到一对合适的α，那么就增大其中一个同时减小另一个。这里所谓的“合适”就是指两个α必须要符合一定的条件，条件之一就是这两个α必须要在间隔边界之外，而其第二个条件则是这两个α还没有进行过区间化处理或者不在边界上。
