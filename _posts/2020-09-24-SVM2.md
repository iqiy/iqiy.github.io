---
layout:     post
title:      "ML学习笔记（一.一）"
subtitle:   "SVM基础2"
date:       2020-09-24 20:33:18
author:     "QIY"
header-img: "img/ML.png"
header-mask: 0.3 
catalog:    true
tags:
    - Machine Learning
---


> Machine Learning 的学习需稳扎稳打

* TOC
{:toc}



# 2.优化方法

=
0$$表示，只有距离最优超平面的支持向量(xi,yi)对应的α非零，其他所有点集的α等于零。综上所述，引入拉格朗日乘子以后，我们的目标变为：

$$
\min_{w,b}\max_{\alpha \geqslant 0}L\left( w,b,\alpha \right)
$$

即先求得α的极大值，再求w和b的极小值。

对偶法：

$$
\max_{\alpha \geqslant 0}\min_{w,b}L(w,b,\alpha)
$$

即先求得w和b的极小值，在求α的极大值。用L(w,b,α)对ww和b分别求偏导，并令其等于0：

$$
\{\begin{matrix}
 & \frac{\partial L(w,b,\alpha)}{\partial w} = 0 \\
 & \\
 & \frac{\partial L(w,b,\alpha)}{\partial b} = 0 \\
\end{matrix}
$$

得：

$$
\{\begin{matrix}
 & w = \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}}x_{{}_{i}} \\
 & \\
 & \sum_{i = 1}^{n}\alpha{}_{i}y_{{}_{i}} = 0 \\
\end{matrix}
$$

把该式代入原来的的拉格朗日式子可得（推导过程省略）：

$$
W\left( \alpha \right) = \sum_{i = 1}^{n}{}\alpha{}_{i} - \frac{1}{2}\sum_{i = 1}^{n}{}\sum_{j = 1}^{n}{}\alpha{}_{i}\alpha{}_{j}y_{{}_{i}}y_{{}_{j}}{x_{{}_{i}}}^{T}x_{{}_{j}}
$$

$$
\sum_{i = 1}^{n}{}\alpha{}_{i}y_{{}_{i}} = 0,\alpha_{{}_{i}} \geqslant 0(i = 1,...,n)
$$

通过SMO方法求a，后求w，b。

$$
w^{*} = \sum_{i = 1}^{n}{}\alpha{{}_{i}}^{*}y_{{}_{i}}x_{{}_{i}}
$$

$$
b^{*} = 1 - {w^{*}}^{T}x_{{}_{s}}
$$

# 3 非线性可分

现实世界的许多问题并不都是线性可分的，尤其存在许多复杂的非线性可分的情形。

![](/img/in-post/200924_svm2/326fdcb0715860f8aca9c9bb1774c5aa.jpg)

要解决这些不可分问题，一般有两种方法。第一种是放宽过于严格的间隔，构造软间隔。另一种是运用核函数把这些数据映射到另一个维度空间去解决非线性问题。

## 3.1 软间隔
## 3.2 核方法

核函数能够恰当的计算给定数据的内积，将数据从输入空间的非线性转变到特征空间，特征空间具有更高甚至无限的维度，从而使得数据在该空间中被转换成线性可分的。如下图所示，我们把二维平面的一组数据，通过核函数映射到了一个三维空间中，这样，我们的超平面就面成了一个平面（在二维空间中是一条直线），这个平面就可以准确的把数据划分开了。

![](/img/in-post/200924_svm2/e9d28967b89baf3f5f38b27c54bcdb10.jpg)

![](/img/in-post/200924_svm2/c669cf8cbfa024d65151cd43fba578a0.jpg)

![](/img/in-post/200924_svm2/27f457c32841ae33edacab1d81692bcd.jpg)

![](/img/in-post/200924_svm2/1e54d00ad9165591182151863a4d5a0f.jpg)

核函数有Sigmoid核、线性核、多项式核和高斯核等，其中高斯核和多项式核比较常用，两种核函数均可以把低维数据映射到高维数据。高斯核的公式如下，σ是达到率，即函数值跌落到0的速度参数：

为二维行向量x1=[x1,x2]x1=[x1,x2]他们的内积为：

𝐾(𝑥1,𝑥2)=(⟨𝑥1,𝑥2⟩)2=(𝑥1𝑥3+𝑥2𝑥4)2=𝑥12𝑥32+2𝑥1𝑥2𝑥3𝑥4+𝑥22𝑥42=𝜙(𝑥1)𝜙(𝑥2)
