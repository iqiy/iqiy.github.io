---
layout:     post
title:      "PRML学习笔记（三）"
subtitle:   "第三章 回归的线性模型"
date:       2018-10-19 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 回归的线性模型

线性回归模型有着可调节的参数，具有线性函数的性质，将会成为本章的关注点。线性回归模型的最简单的形式也是输入变量的线性函数。但是通过将一组输入变量的非线性函数进行线性组合，我们可以获得一类更加有用的函数：基函数。这样的模型是参数的线性函数，但是输入变量的非线性函数。

## 3.1 线性基函数模型

将输入变量的固定非线性函数进行线性组合，形式为：

$$ y(x, w) = w_{0} + \sum_{j=1}^{M-1}w_{j}\phi_{j}(x) $$

其中$ \phi_{j}(x)$被称为基函数。通过把下标j的最大值记做M-1，这个模型中的参数总数为M。参数$w_{0}$使得数据中可以存在任意固定的偏置，这个值通常被称为偏置参数。

通常定义一个额外的虚“基函数”$\phi_{0}(x) = 1$是很方便的，这时：

$$ y(x, w) = \sum_{j=1}^{M-1}w_{j}\phi_{j}(x) = w^{T}\phi(x) $$

其中$w = w({0}, \dots, w_{M-1})^{T}$且$\phi = (\phi_{0}, \dots, \phi_{M-1})^{T}$。在许多模式识别的实际应用中，我们会对原始的数据变量进行某种固定形式的预处理或特征抽取。如果原始变量由向量x组成，那么特征可以用基函数${\phi_{j}(x)}$表示。

对于基函数的选取由很多种选择，如多项式基函数$\phi_{j}(x)=x^{j}$、高斯基函数：

$$ \phi_{j}(x) = exp \left\{ -\frac{(x-\mu_{j})^{2}}{2s^{2}} \right\} $$

其中$\mu_{i}$控制了基函数在输入空间的位置，参数s控制了基函数空间的大小。

另一种该选择是sigmoid基函数，形式为：

$$ \phi_{j}(x) = \sigma\left(\frac{x-\mu_{j}}{s}  \right) $$

其中$\sigma(a)$是logistic sigmoid函数，定义为：

$$ \sigma_{a} = \frac{1}{1 + exp(-a)} $$

等价地，我们还可以使用tanh函数，因为它和sigmoid函数的关系为$tanh(a) = 2\sigma(2a) - 1$，因此sigmoid函数的一般线性组合等价于tanh函数的一般线性组合。

还有很多其他的基函数，如傅里叶基函数等。

### 3.1.1 最大似然与最小平方

这里我们更详细地考虑最小平房的方法以及它与最大似然方法的关系。

与之前一样，我们假定目标变量t由确定的函数y(x,w)给出，这个函数被附加了高斯噪声，即：

$$ t = y(x,w) + \epsilon $$

其中$\epsilon$是一个零均值的高斯随机变量，精度(方差的倒数)为$\beta$，因此我们有：

$$ p(t | x, w, \beta) = N(t | y(x,w), \beta^{-1}) $$

如果我们假设一个平方损失函数，那么对于x的一个新值，最优的预测由目标变量的条件均值给出。在上述高斯分布的情况下，条件均值可简单地写为：

$$ E[t|x] = \int tp(t | x)dt = y(x, w) $$

假设数据集X包含N个数据点x，它们独立从上述分布去除，那么可写出似然函数表达式：

$$ p(t|X, w, \beta) = \prod_{n=1}^{N}N(t_{n} | w^{T}\phi(x_{n}), \beta^{-1}) $$

在有监督学习中，x总会出现在条件变量位置上，因此下面公式中将其省略掉而不会引起歧义。对似然函数取对数有：

$$ ln p(t | w, \beta) = \sum_{n=1}^{N} ln N(t_{n} | w^{T}\phi(x_{n}), \beta^{-1}) $$

其中平方和误差函数的定义为：

$$ E_{D}(w) = \frac{1}{2}\sum_{n=1}^{N}\{ t_{n} - w^{T}\phi(x_{n}) \}^{2} $$

写出了最大似然函数，我们可以使用最大似然的方法确定w和$\beta$。我们看到，在条件高斯噪声分布的情况下，线性模型的似然函数的最大化等价于误差函数的最小化。平方和误差函数由$E_{D}(w)$给出。w的最大似然解为：

$$ w_{ML} = (\Phi^{T}\Phi)^{-1}\Phi^{T}t $$

这被称为最小平房问题的规范方程。这里的$\Phi$是一个零均值的高斯随机变量$N\times M$的矩阵，被称为设计矩阵。它的元素为$\phi_{nj} = \phi_{j}(x_{n})$.即：

$$
\Phi = \left(
    \begin{aligned}
    \phi_{0}(x_{1}) && \phi_{1}(x_{1}) && \dots && \phi_{M-1}(x_{1}) \\
    \phi_{0}(x_{2}) && \phi_{1}(x_{2}) && \dots && \phi_{M-1}(x_{2}) \\
    \vdots          && \vdots          && \ddots&& \vdots            \\
    \phi_{0}(x_{N}) && \phi_{1}(x_{N}) && \dots && \phi_{M-1}(x_{N})
\end{aligned}
    \right)
$$

量 $$ \Phi^{\dag} \equiv (\Phi^{T}\Phi)^{-1}\Phi^{T} $$ 被称为矩阵$\Phi$的Moore-Penrose伪逆矩阵，它可以看做为逆矩阵的概念对于非方阵的推广。

现在可以更加深刻的认识偏置参数$w_{0}$。如果我们显示的写出偏置参数，那么误差函数变为：

$$ E_{D}(w) = \frac{1}{2}\sum_{n=1}^{N}\{ t_{n} - w_{0} - \sum_{j=1}^{M-1}w_{j}\phi_{j}(x_{n}) \}^{2} $$

令关于$w_{0}$的导数等于零，求解出$w_{0}$可得：

$$ w_{0} = \bar{t} - \sum_{j=1}^{M-1}w_{j}\bar{\phi}_{j} $$

其中我们已经定义了：

$$ \bar{t} = \frac{1}{N}\sum_{n=1}^{N}t_{n}~~,~~~~~\bar{\phi}_{j} = \frac{1}{N}\sum_{n=1}^{N}\phi_{j}(x_{n}) $$

因此**偏置$w_{0}$不唱了目标值的平均值(在训练数据集上的)与基函数的值的平均值的加权求和之间的差**。

我们也可以关于噪声精度参数$\beta$最大化似然函数，结果为：

$$ \frac{1}{\beta_{ML}} = \frac{1}{N}\sum_{n=1}^{N}\{ t_{n} - w_{ML}^{T}\phi(x_{n}) \}^{2} $$

因此我们看到**噪声精度的倒数由目标值在回归函数周围的残留方差给出**。

### 3.1.4 正则化最小平方

考虑平方和误差函数，对其添加一个通用的正则化项，则此时正则化的误差函数的形式为：

$$ \frac{1}{2}\sum_{n=1}^{N}\{ t_{n} - w^{T}\Phi(x_{n}) \}^{2} + \frac{\lambda}{2}\sum_{j=1}^{M}|w_{j}|^{q} $$

**当q=2时对应二次正则化项。在统计学文献中，q=1的情形被称为lasso。它的性质为：如果$\lambda$充分大，那么某些系数$w_{j}$会变为零，从而产生了一个稀疏模型，这个模型中对应的基函数不起作用**。

### 3.1.5 多个输出

当我们想预测K>1个目标变量时，对于目标t的每个分量，引入一个不同的基函数集合，从而变成了独立的回归问题。但是，一个更有趣的并且更常用的方法是对目标向量的所有分量使用一组相同的基函数来建模，即：

$$ y(x,w) = W^{T}\phi(x) $$

其中y是一个K维列向量，W是一个$M\times K$的参数矩阵，$\phi(x)$是一个M维列向量。假设我们令目标向量的条件概率分布是一个各项同性的高斯分布，形式为：

$$ p(t | x, W, \beta) = N(t | W^{T}\phi(x), \beta^{-1}I) $$

与之前一样，我们可以关于W的最大化这个函数，可得最大似然解：

$$ W_{ML} = (\Phi^{T}\Phi)^{-1}\Phi^{T}T $$

## 3.2 偏置-方差分解

前面我们说过过拟合现象是最大似然方法的一个不好的性质，当时当我们在使用贝叶斯方法对参数进行求和或积分时，过拟合现象不会出现。频率学家的观点有一个被称为piancha-方差折中。我们基于线性基函数模型来说明一些基本思想，但实际上这种讨论具有更加普遍的适用性。

如果我们使用由参数向量w控制的函数$y(x,w)$对条件期望
$$h(x) =E[t|x]$$建模，那么从贝叶斯的观点来看，我们模型的不确定性是通过w的后验概率分布来表示的。但是频率学家的方法涉及到根据数据集D对w进行点估计，然后试着通过下面的思想实验来表示估计的不确定性：假设我们由许多数据集，每个数据集的大小为N，并且每个数据集都独立地从分布$p(t,x)$中抽取。对于任意给定的数据集D，我们可以运行我们的算法，得到一个预测函数$y(x;D)$。不同的数据集会给出不同的函数，从而给出不同的平方损失的值。这样，特定的学习算法的表现就可以通过取各个数据集上表现的平均值进行评估。

$$ E_{D}[\{ y(x;D) - h(x) \}^{2} ] = \{ E_{D}[y(x;D)] - h(x) \}^{2} + E_{D}[\{y(x;D) - E_{D}[y(x;D)] \}^{2} ] $$

**我们看到，$y(x;D)$与回归函数h(x)的差的平方的期望可以表示为两项的和。第一项被称为平方偏置，表示所有数据集的平均预测与预期的回归函数之间的差异。第二项被称为方差，度量了对于单独的数据集，模型所给出的解在平均值附近波动的情况，因此也就度量和函数$y(x;D)$对于特定数据集的选择的敏感度**。

到现在为止，我们已经考虑了单一输入变量x的情形。如果把这个展开式带到平方损失函数的期望中，那么我们就得到了下面的对于期望平方损失的分解：

$$ 期望损失 = 偏置^{2} + 方差 + 噪声 $$

其中：

$$ 偏置^{2} = \int \{E_{D}[y(x;D)] - h(x)\}^{2}p(x) dx $$

$$ 方差 = \int E_{D}[\{y(x;D) - E_{D}[y(x;D)]\}^{2}]p(x) dx $$

$$ 噪声 = \int\int\{ h(x) - t\}^{2}p(x,t) dxdt $$

我们的目标是最小化期望损失，它可以被分解为偏置、方差和一个常数噪声项的和。因此**偏差和方差之间由一个这种。对于非常灵活的模型来说，偏置较小而方差较大。对于相对固定的模型来说，偏置较大，方差较小。有着最有预测能力的模型时在偏置和方差之间取得最优的平衡的模型**。

## 3.3 贝叶斯线性回归

这里我们考虑线性回归的贝叶斯方法，这回避免最大似然的过拟合问题。

### 3.3.1 参数分布

关于线性拟合的贝叶斯方法的讨论，我们首先引入模型参数w的先验分布概率。此时我们把噪声精度参数$\beta$当做已知参数(未知的话可以引入高斯-Gamma分布作为先验分布)。：

$$ p(w) = N(w | m_{0}, S_{0}) $$

接下来我们计算后验分布，它正比于似然函数与先验分布的乘积：

$$ p(w|t) = N(w | m_{N}, S_{N}) $$

其中

$$ m_{N} = S_{N}(S_{0}^{-1}m_{0} + \beta\Phi^{T}t) $$

$$ S_{N}^{-1} = S_{0}^{-1} + \beta\Phi^{T}\Phi $$

对于本章的剩余部分，为了简化起见，我们将考虑高斯先验的一个特定的形式。具体来说，我们考虑零均值各向同性的高斯分布，这个分布由一个精度参数$\alpha$控制，即：

$$ p(w | \alpha) = N(w | 0, \alpha^{-1}I) $$

其中

$$ m_{N} = \beta S_{N}\Phi^{T}t $$

$$ S_{N}^{-1} = \alpha I + \beta \Phi^{T}\Phi $$

### 3.3.2 预测分布

在实际应用中，我们感兴趣的通常是对于新的x的值预测出t的值。这需要我们计算出预测分布，定义为：

$$ p(t | \mathbf{t}, \alpha, \beta) = \int p(t | w, \beta)p(w | \\mathbf{t}, \alpha, \beta) dw $$

可以看出，上式是两个高斯分布的卷积，因此可直接由公式得到预测分布的形式为：

$$ p(t | \mathbf{x}, \mathbf{t}, \alpha, \beta) = N(t | m_{N}^{T}\phi(x), \sigma_{N}^{2}(x)) $$

其中预测分布的方差$\sigma_{N}^{2}$为：

$$ \sigma_{N}^{2}(x) = \frac{1}{\beta} + \phi(x)^{T}S_{N}\phi(x) $$

上式第一项表示数据中的噪声，而第二项反应了与参数w关联的不确定性。当N趋于无穷时，第二项趋近于0，从而预测分布的方差只与参数$\beta$控制的具有可加性的噪声有关。

### 3.3.3 等价核

首先说一下核方法的含义，往简单里说，核方法是将一个低维的线性不可分的数据映射到一个高维的空间、并期望映射后的数据在高维空间里是线性可分的。

对预测均值进行变形：

$$ y(x, m_{N}) = m_{N}^{T}\phi(x) = \beta\phi(x)^{T}S_{N}\Phi^{T}t = \sum_{n=1}^{N}\beta\phi(x)^{T}S_{N}\phi(x_{n})t_{n} $$

因此，在点x除预测均值由训练集目标变量$t_{n}$的线性组合给出，定义为$k(x, x^{'})$：

$$ k(x, x^{'}) = \beta\phi(x)^{T}S_{N}\phi(x^{'}) $$

k被称为平滑矩阵或等价核。像这样的回归函数，通过对训练集里目标值进行线性组合做预测，被称为线性平滑。

用核函数表示线性回归给出了解决问题的另一种方法。我们不引入一组基函数(它隐式地定义了一个等价核)，而是直接定义了一个局部分核函数，然后在给定观测数据集的条件下，使用这个核函数对新的输入变量x做预测。这就引出了一个很实用的框架，叫高斯过程。

可以看到，一个等价核定义了模型的权值。通过这个权值，训练集里的目标值被组合，然后对新的x做预测。可以证明k满足归一化条件。

## 3.4 贝叶斯模型比较

这里我们通过贝叶斯方法比较模型。模型比较的贝叶斯观点仅仅涉及到使用概率来表示模型选择的不确定性，以及恰当地使用概率的加和规则和乘积规则。

假设我们想比较L个模型${M_{i}}$，这里一个模型指的是观测数据D上的概率分布。我们假定不确定性通过先验概率分布$p(M)$表示。给定一个训练数据集，我们想估计后验分布：

$$ p(M_{i} | D) \propto p(M_{i})p(D | M_{i}) $$

先验分布让我们能够表达不同模型之间的优先级，现在简单滴假设所有模型都有相同的先验概率。
$p(D | M_{i})$被称为模型证据(model evidence)，它表达了数据表现出的不同模型的优先级。模型证据有时也被称为边缘似然，因为他可以被看做在模型空间中的似然函数，在这个空间中参数已经被求和或积分，两个模型证据的比值被称为贝叶斯因子。

对于一个由参数w控制的模型，根据概率加和规则和乘积规则，模型证据为：

$$ p( D | M_{i} ) = \int p(D|w, M_{i})p(w|M_{i})dw $$

从取样的角度来看，边缘似然函数可以被看成从一个模型中生成数据集D的概率，这个模型的参数是从先验分布中随机取样的。

通过对参数的积分进行一个简单的近似，我们可以更加深刻地认识模型证据。首先考虑模型由一个参数w的情形，这个参数的后验概率正比于
$p(D|w)p(w)$，其中为了简化记号，我们省略它对模型$M_{i}$的依赖。如果我们假定后验分布在最大似然值$w_{MAP}$附近是一个尖峰，宽度为$\Delta w_{后验}$，那么我们可以用被积函数的值乘以尖峰的宽度来近似这个积分。如果我们进一步假设先验分布是平的，宽度为$\Delta w_{先验}$，即$p(w) = \frac{1}{\Delta w_{先验}}$，那么我们有：

$$ p(D) = \int p(D |w)p(w)dw \simeq p(D|w_{MAP})\frac{\Delta w_{后验}}{\Delta w_{先验}} $$

取对数可得：

$$ \ln p(D) \simeq \ln p(D | w_{MAP}) + \ln(\frac{\Delta w_{后验}}{\Delta w_{先验}}) $$

第一项表示拟合由最可能参数给出的数据。对于平的先验分布来说，对应于对数似然。第二项根据模型的复杂度来惩罚模型。由于$\Delta w_{后验}< \Delta w_{先验}$，因此这一项为负，并且随着$\frac{\Delta w_{后验}}{\Delta w_{先验}}$的减小，它的绝对值会增加。因此如果参数精确地调整为后验分布的数据，那么惩罚项会很大。

对于一个有M个参数的模型，我们可以对每个参数进行类似的近似。假设所有的参数的$\frac{\Delta w_{后验}}{\Delta w_{先验}}$都相同，我们有：

$$ \ln p(D) \simeq p(D|w_{MAP}) + M\ln(\frac{\Delta w_{后验}}{\Delta w_{先验}}) $$

因此，在这种非常简单的近似下，复杂度的惩罚项的代销随着模型中可调节参数M的数量线性增加。随着我们增加模型的复杂度，第一项通常会很大，依次为一个更加复杂的模型能够更好地拟合数据。而第二项会减小，因为它依赖于M。由最大证据确定的最优模型复杂度需要在这两个相互竞争的项之间进行折中。

## 3.5 证据近似

在处理现行基函数模型的纯粹贝叶斯方法中，我们会引入超参数$\alpha$和$\beta$的先验分布，然后通过对超参数以及参数w求积分的方式做预测。但是，虽然我们可以解析第求出对w的积分或者求出对超参数的积分，但是对所有这些变量完整地求积分是没有解析解的。

这里我们讨论一种近似方法，在该方法中，我们首先对参数w求解分，得到边缘似然函数，然后通过最大化边缘似然函数，确定超参数的值。这个框架在统计学的文献中被称为经验贝叶斯，或者被称为第二类最大似然。

如果我们引入$\alpha$和$\beta$上的超先验分布，那么预测分布可以通过对w，$\alpha$和$\beta$求积分的方法得到：

$$ p(t | \mathbf{t}) = \int\int\int p(t | w, \beta)p(w | \mathbf{t},\alpha,\beta)p(\alpha,\beta | \mathbf{t}) dw d\alpha d\beta $$

如果后验分布
$p(\alpha, \beta | \mathbf{t})$在$\hat{\alpha}$和$\hat{\beta}$附近由尖峰，那么预测分布可以通过对w积分的方式简单滴得到，其中$\alpha$和$\beta$被固定为$\hat{\alpha}$和$\hat{\beta}$，即：

$$ p(t | \mathbf{t}) \simeq p(t | \mathbf{t}, \hat{\alpha}$, $\hat{\beta}) = \int p(t|w,\hat{\beta}p(w | \mathbf{t}, \hat{\alpha}$, $\hat{\beta})) $$

根据贝叶斯定力，$\alpha$和$\beta$的后验分布为：

$$ p(\alpha, \beta | \mathbf{t}) \propto p(\mathbf{t} | \alpha, \beta)p(\alpha, \beta) $$

如果先验分布相对比较平，那么在证据框架中，
$\hat{\alpha}$和$\hat{\beta}$可以通过最大化边缘似然函数
$p(\mathbf{t} | \alpha, \beta)$来获得。


### 3.5.1 计算证据函数

边缘似然函数可以被看成从一个模型中生成数据集
$p(\mathbf{t} | \alpha, \beta)$是通过对权值w进行积分得到的，即：

$$ p(\mathbf{t} | \alpha, \beta) = \int p(\mathbf{t} | w, \beta)p(w| \alpha) dw $$

之后我们通过对指数项陪平方，而后使用高斯分布的归一化系数的基本形式来获得边缘似然函数的对数形式：

$$ \ln p(\mathbf{t} | \alpha, \beta) = \frac{M}{2}\ln\alpha + \frac{N}{2}\ln\beta - E(m_{N}) - \frac{1}{2}\ln|A| - \frac{N}{2}\ln(2\pi) $$

通过观察模型证据对M的变化图像，可以选取最佳的模型复杂度参数M。

### 3.5.2 最大化证据函数

首先考虑模型由一个参数
$p(\mathbf{t} | \alpha, \beta)$关于$\alpha$的最大化。首先定义下面的特征向量方程：

$$ (\beta\Phi^{T}\Phi)u_{i} = \lambda_{i}u_{i} $$

其中A的特征值为$\alpha + \lambda_{i}$。现在考虑边缘似然函数的对数中涉及到
$\ln|A|的项关于$\alpha$的导数：

$$ \frac{d}{d\alpha}\ln|A| = \frac{d}{d\alpha}\ln\prod_{i}(\lambda_{i} + \alpha) = \frac{d}{d\alpha}\sum_{i}\ln(\lambda_{i} + \alpha) = \sum_{i}\frac{1}{\lambda_{i} + \alpha} $$

令导数为0可得：

$$ \alpha m_{N}^{T}m_{N} = M - \alpha\sum_{i}\frac{1}{\lambda_{i} + \alpha} = \gamma$$

由于i的求和式共有M项，因此$\gamma$可以写为：

$$ \gamma = \sum_{i} \frac{\lambda_{i}}{\alpha + \lambda_{i}} $$

因此，我们看到最大化边缘似然函数的$\alpha$满足：

$$ \alpha = \frac{\gamma}{m_{N}^{T}m_{N}} $$

这是一个$\alpha$的隐式解，不仅因为与$\alpha$和$\gamma$有关，还因为后验概率本身的众数$m_{N}$也和$\alpha$的选择有关。因此我们使用迭代的方法求解，如EM算法。

对$\beta$的最大化对数边缘似然函数解方法类似，结果为：

$$ \frac{1}{\beta} = \frac{1}{N-\gamma}\sum_{n=1}^{N}\{ t_{n}-m_{N}^PT \phi(x_{n})\} $$

也需要通过迭代的方法来求解。
