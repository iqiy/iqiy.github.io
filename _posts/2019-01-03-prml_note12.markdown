---
layout:     post
title:      "PRML学习笔记（十二）"
subtitle:   "第十二章 连续潜在变量"
date:       2019-01-03 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 连续潜在变量

## 12.1 主成分分析

主成分分析，或者称为PCA，是一种被广泛使用的技术，应用领域包括维度降低。有损数据压缩、特征抽取、数据可视化。它也被成为 Karhunen-Loeve 变换。有两种经常使用的PCA的定义，他们会给出同样的算法。**PCA可以被定义为数据在低维线性空间上的正交投影，这个线性空间被称为主子空间，使得投影数据的方差被最大化。等价地，它也可以被定义为使得平均投影代价最小的线性投影。平均投影代价是指数据点和它们投影之间的平均平方距离**。

### 12.1.1 最大方差形式

假设有一组观测数据集$x_{n}$，维度为D，目标是将数据投影到维度为M(M<D)的空间中，同时最大化投影数据的方差。

考虑在一维空间上的投影，我们可以使用D维向量$u_{1}$定义这个空间的方向。为了方便，我们假定选择一个单位向量，这样，每个数据点$x_{n}$被投影到一个标量值$u_{1}^{T}x_{n}$上。投影数据的均值是$u_{1}^{T}\bar{x}$，投影数据的方差为：

$$ \frac{1}{N}\sum_{n=1}^{N}\{u_{1}^{T}x_{n} - u_{1}^{T}\bar{x}\}^{2} = u_{1}^{T}Su_{1} $$

我们现在关于$u_{1}$最大化投影方差$u_{1}^{T}Su_{1}$。采用拉格朗日乘数法，以u的归一化条件为限制，我们看到驻点满足：

$$ Su_{1} = \lambda_{1}u_{1} $$

这表明$u_{1}$一定是S的一个特征向量。如果我们左乘$u_{1}^{T}$，使用$u_{1}^{T}u_{1}=1$，我们看到方差为：

$$u_{1}^{T}Su_{1} = \lambda_{1} $$

因此当我们**将$u_{1}$设置为与最大的特征值$\lambda_{1}$的特征向量相等时，方差会达到最大值，这个特征向量被称为第一主成分**。对于其他主成分，我们可以考虑那些与现有方向正交的所有可能方向中，将新的方向选择为最大化投影方差的方向。以此类推得到协方差矩阵S的M个特征向量$u_{1},\dots,u_{M}$，对应于M个最大特征值$\lambda_{1},\dots,\lambda_{M}$。

### 12.1.2 最小误差形式

现在考虑基于误差最小化的投影方法。为此我们引入D维基向量的一个完整的单位正交集合$u_{i}$，其中$i=1,\dots,D$，且满足：

$$ u_{i}^{T}u_{j} = \delta_{ij} $$

由于基是完整的，因此数据点可以表示为基向量的线性组合，因此：

$$ x_{n} = \sum_{i=1}^{D}(x_{n}^{T}u_{i})u_{i} $$

然而，我们的目标是使用限定数量M个变量的一种表示方法来近似数据点，这对应于在低维子空间上的投影，若采用M个基向量来表示M为子空间，那么我们可以用下式来近似每个数据点：

$$ \tilde{x}_{n} = \sum_{i=1}^{M}z_{ni}u_{i} + \sum_{i=M+1}^{D}b_{i}u_{i} $$

其中${z_{ni}}$依赖于特定的数据点，而 $b_{i}$是常数，对于所有的数据点都相同。为了最小化失真，我们采用原始数据点和与它近似点$\tilde{x}_{n}$之间的平方距离，在数据集上取平均，即最小化下式：

$$ J = \frac{1}{N}\sum_{n=1}^{N}||x_{n} - \tilde{x}_{n} ||^{2} $$

消去上式中的$z_{ni}$和$b_{i}$，则得到纯粹关于$u_{i}$的J的表达式：

$$ J = \sum_{i=M+1}^{D}u_{i}^{T}Su_{i} $$

对于任意的D和M<D，最小化J的解一般都可以通过将$u_{i}$选择为协方差矩阵的特征向量的方式的得到，即：

$$ Su_{i} = \lambda_{i}u_{i} $$

这样，J就变成：

$$ J = \sum_{i=M+1}^{D}\lambda_{i} $$

这就是与主子空间正交的特征值的加和，于是，我们可以通过将这些特征向量选择成D-M个最小的特征值对应的特征向量，来得到J的最小值，因此**定义了主子空间的特征向量是对应于M个最大特征值的特征向量**。

## 12.2 概率PCA

PCA也可以被视为概率潜在变量模型的最大似然解，PCA的这种形式被称为概率PCA，它与因子分析密切相关。

概率PCA是线性高斯框架的一个简单的例子，其中所有的边缘概率分布和条件概率分布都是高斯分布。首先显示引入潜在变量z，对应于主成分子空间。接下来我们定义潜在变量上的一个高斯先验分布p(z)以及高斯条件概率分布
$p(x|z)$：

$$ p(z) = \mathcal{N}(z | 0, I) $$

$$ p(x | z) = \mathcal{N}(x | Wz + \mu, \sigma^{2}I) $$

其中x的均值是z的一个一般线性函数，由$D\times M$的矩阵W和D维向量$\mu$控制。W的列张成了数据空间的一个线性子空间，对应于主子空间。p(z)被定义为零均值单位协方差的高斯是因为更一般的高斯分布会产生一个等价的概率模型。

假如从生成式的观点看待概率PCA模型的话，观测值的一个采样值可以这样获得：首先为潜在变量选择一个值，然后以这个潜在变量的值为条件，对观测变量采样。具体来说，D维观测变量x由M维潜在变量z的一个线性变换附加一个高斯“噪声”定义，即：

$$ x = Wz + \mu + \epsilon $$

其中$\epsilon$是一个D维零均值高斯分布的噪声变量。可以看出，这个框架基于的是从潜在空间到数据空间的一个映射，从数据空间到潜在空间的逆映射可以通过使用贝叶斯定理的方式得到。

我们希望使用最大似然的方式确定$W, \mu, \sigma^{2} $的值。概率PCA模型可以表示为一个有向图，如下图所示，则对应的对数似然函数为：

$$ \ln p(X | mu, W, \sigma^{2}) = \sum_{n=1}^{N}\ln p(x_{n} | W, \mu, \sigma^{2}) = -\frac{ND}{2}\ln(2\pi) - \frac{N}{2}\ln|C| - \frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{T}C^{-1}(x_{n}-\mu) $$

![](/img/in-post/prml_note8/p13.png)

$\mu$的解及W和$\sigma^{2}$的近似封闭解为：

$$ \mu = \bar{x} $$

$$ W_{ML} = U_{M}(L_{M} - \sigma^{2}I)^{\frac{1}{2}}R $$

$$ \sigma^{2}_{ML} = \frac{1}{D-M}\sum_{i=M+1}^{D}\lambda_{i} $$

其中$U_{M}$是一个$D\times M$的矩阵。当M个特征向量被选为前M个最大的特征值所对应的特征向量时，对数似然函数可以达到最大值，其他所有的解都是鞍点。假定特征向量按照对应的特征值的大小降序排列，从而M个主特征向量是$u_{1},\dots,u_{M}$，从而W的列定义了标准PCA的主子空间。而$\sigma_{ML}^{2}$是与丢弃的维度相关联的平均方差，它可以被看做是M为潜在空间的一个旋转矩阵。

### 12.2.4 因子分析

因子分析是一个线性高斯潜在变量模型，它与概率PCA密切相关。**它的定义与概率PCA唯一的差别是给定潜在变量z的条件下观测变量x的条件概率分布的协方差矩阵是一个对角矩阵而不是各项同性的协方差矩阵**，即：

$$ p(x | z) = \mathcal{N}(x | Wz + \mu, \Psi) $$

其中$\Psi$是一个$D\times D$的对角矩阵。本质上讲，**因子分析模型对数据的观测协方差的结构解释为：表示出矩阵$\Psi$中与每个坐标相关联的独立变量，然后描述矩阵W中的变量之间的协方差**。

从潜在变量密度模型角度来看因子分析，我们感兴趣的是潜在空间的形式，而不是描述它的具体的坐标系的选择。**如果我们想要移除与潜在空间旋转相关联的模型的退化，那么我们必须考虑非高斯的潜在变量分布，这就产生了独立成分分析(ICA)模型**。

## 12.3 核PCA

若我们将核替换的方法应用到主成分分析中，从而得到了一个非线性的推广，被称为核PCA(kernel PCA)。我们希望避免直接在特征空间中进行计算，因此我们完全根据核函数来建立算法的公式。在中心化之后，投影的数据点为：

$$ \tilde{\phi}(x_{n}) = \phi(x_{n}) - \frac{1}{N}\sum_{l=1}^{N}\phi(x_{l}) $$

从而Gram矩阵的对应元素为：

$$
\begin{aligned}
\tilde{K}_{nm} = & \tilde{\phi})(x_{n})^{T}\tilde{\phi}(x_{m}) \\
        = & \phi(x_{n})^{T}\phi(x_{m}) - \frac{1}{N}\sum_{l=1}^{N}\phi(x_{n})^{T}\phi(x_{l})-\frac{1}{N}\sum_{l=1}^{N}\phi(x_{l})^{T}\phi(x_{m}) - \frac{1}{N^{2}}\sum_{j=1}^{N}\sum_{l=1}^{N}\phi(x_{j})\phi(x_{l}) \\
        = & k(x_{n}, x_{m}) - \frac{1}{N}\sum_{l=1}^{N}k(x_{l}, x_{m}) - \frac{1}{N}\sum_{l=1}^{N}k(x_{n}, x_{l}) + \frac{1}{N^{2}}\sum_{j=1}^{N}\sum_{l=1}^{N}k(x_{j}. x_{l}) 
\end{aligned}
$$

因此，**我们可以只使用核函数来计算$\tilde{K}$，然后使用$\tilde{K}$确定特征值和特征向量。注意，如果我们使用线性核$k(x, x^{'}) = x^{T}x^{'}$，那么我们就恢复了标准的PCA算法**。
