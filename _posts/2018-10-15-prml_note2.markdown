---
layout:     post
title:      "PRML学习笔记（二）"
subtitle:   "第二章 概率分布"
date:       2018-10-15 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 概率分布

## 2.1 二元变量
### 2.1.0.1 伯努利分布

在给定有限次观测数据的前提下，对随机变量x的概率分布p(x)进行建模。这个问题被称为密度估计。分布函数是直接和概率测度相关的，而密度函数需要另一个测度参与，而且不一定存在。

当一个变量的取值只有两种可能，如0或1这种时。其中取1的概率被记做$\mu$。因此：

$$ p(x = 1 | \mu) = \mu $$

因此x的概率分布就是伯努利分布，公式表达为：

$$ Bern(x | \mu) = \mu^{x}(1-\mu)^{1-x} $$

其均值和方差为：

$$ E[x] = \mu $$

$$ var[x] = \mu(1-\mu) $$

假设观测数据是i.i.d.的，那么对应的似然函数为：

$$ p(D | \mu) = \prod_{n=1}^{N}p(x_{n} | \mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{1-x_{n}} $$

通过对对数似然求导得到概率的最大似然估计值：

$$ \mu_{ML} = \frac{1}{N}\sum_{n=1}{N}x_{n} $$

### 2.1.0.1 二项分布

二项分布简单来说就是进行多次实验的伯努利分布。学术上来讲它是求解在给定数据集规模N的条件下，x=1的观测出现数量m的概率分布。公式表述为：

$$ Bin(m | N,\mu) = \frac{N!}{(N-m)!m!}\mu^{m}(1-\mu)^{N-m} $$

对于独立的事件，加和的均值等于均值的加和，嘉禾的方差等于方差的加和。

### 2.1.1 Beta分布

现在我们要求解二项分布中$\mu$的最大似然解，即数据集中x=1出现的比例。为了从贝叶斯的角度看待这个问题，我们需要引入一个关于$\mu$的先验分布$p(\mu)$。

这里首先介绍一下**共轭性，它值一个概率分布的后验概率与先验分布具有相同的函数形式，它能使贝叶斯分析得到极大的简化**。

对于二项分布来说，由于似然函数为$\mu^{x}(1-\mu)^{1-x}$的形式，那么对应的先验分布也得具有这个形式才能满足共轭性，因此我们选择先验分布为Beta分布：

$$ Beta(\mu | a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1} $$

其中a，b为超参数。Beta分布对应的均值和方差为：

$$ E[\mu] = \frac{a}{a + b} $$

$$ var[\mu] = \frac{ab}{(a+b)^{2}(a + b + 1)} $$

因为先验函数与后验函数满足共轭性，因此后验函数的概率分布为(已归一化)：

$$ p(\mu | m,l,a,b) = \frac{\Gamma(m+a+l+b)}{\Gamma(m+a)\Gamma(l+b)}\mu^{m+a-1}(1-\mu)^{l+b-1} $$

如果我们的目标是尽可能好的预测下一次实验的的输出，那么我们必须估计给定观测数据集D的情况下，x的预测分布。其形式为：

$$ p(x = 1 | D) = \int_{0}^{1}p(x=1 | \mu)p(\mu | D)d\mu = \int_{0}^{1}\mu p(\mu | D)d\mu = E[\mu | D] $$

将前面求得的对应项带入得：

$$ p(x = 1 | D) = \frac{m+a}{m+a+l+b} $$

可以看到，当m,l趋于无穷大时，上式变成了最大似然的结果。因此**贝叶斯的结果和最大似然的结果在数据集的规模区域无穷的情况下会统一到一起**。

并且由Beta分布的方差可知，**在平均来看，随着我们观测到越来越多的数据，后验概率的不确定性将会持续的下降**。公式表示为：

$$ E_{\theta}(\theta) = E_{D}[E_{\theta}[\theta | D]] $$

$$ var_{\theta}[\theta] = E_{D}[var_{\theta}[\theta | D]] + var_{D}[E_{\theta}[\theta | D]] $$

可以看到，**$\theta$的后验方差小于先验的方差。后验均值的方差越大，这个方差的减小就越大**。需要注意的是上式为平均结果，对于一个特定的数据集，有可能后验方差大于先验方差。

## 2.2 多项式变量

二元变量可以用来描述只能取两种可能值中的某一种这样的量。但对于可以取K个互斥状态中的某一个离散变量时，我们需要采用"1-of-K"表示法(类似于one-hot)，则：

$$ x = (0, 0, \dots, 1, \dots, 0)^{T} $$

若用参数$\mu_{k}$表示$x_{k}=1$的概率。那么x的分布就是：

$$ p(x|\mu) = \prod_{k=1}^{K}\mu_{k}^{x_{k}} $$

上式可以看做伯努利分布对于多个输出的一个推广。该分布的均值为：

$$ E[x | \mu] = \sum_{x}p(x | \mu)x = (\mu_{1}, \dots, \mu_{K})^{T} $$

若数有N个独立的观测值数据集，那么对应的似然函数就是各个数据点概率的连乘。

$$ p(D | \mu) = \prod_{k=1}^{K}\mu_{k}^{m_{k}} $$

$\mu$的最大似然解可以通过拉格朗日乘数法得到，其限制条件为$\mu_{k}$的和必须为1：

$$ \sum_{k=1}^{K}m_{k}ln\mu_{k} + \lambda(\sum_{k=1}^{K}\mu_{k}-1) $$

令上式导数为0并将限制条件带入可得最大似然解：

$$ \mu_{k}^{ML} = \frac{m_{k}}{N} $$

它是N次观测中，$x_{k}=1$所占的比例。

当我们考虑
$ m_{1}, m_{2}, \dots, m_{K}$在参数$\mu$和观测总数N条件下的联合分布。根据上面的似然函数公式，我们有：

$$ Mult(m_{1}, m_{2}, \dots, m_{K} |\mu, N) = \frac{N!}{m_{1}!m_{2}!\dots m_{K}!} $$

其中 $\sum_{k=1}^{K}m_{k} = N $。

多项式分布的参数$\mu_{k}$的先验分布为狄利克雷分布：

$$ DIr(\mu | \alpha) = \frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{1})\dots\Gamma(\alpha_{K})} \prod_{k=1}^{K}\mu_{k}^{\alpha_{k-1}} $$

其中$\alpha$是分布的参数: 

$$\alpha_{0} = \sum_{k=1}^{K}\alpha_{k} $$

用似然函数乘以先验就得到了参数的后验分布(已归一化)：

$$ p(\mu | D,\alpha) = Dir(\mu | \alpha + m) $$

## 2.3 高斯分布

高斯分布的简单定义和性质这里不在赘述，下面我们考虑高斯分布的几何形式。高斯对于x的依赖是通过下面形式的二次型：

$$ \Delta^{2} = (x-\mu)^{T}\Sigma^{-1}(x-\mu) $$

首先我们将$\Sigma$取为对称矩阵，而不失一般性。这是因为任何非对称项都会从指数中消失。那么协方差矩阵的特征向量方程可以写作：

$$ \sum\mu_{i} = \lambda_{i}\mu_{i} $$

我们可以选取特征向量为正交的，因此协方差矩阵可以表示成特征向量的展开的形式：

$$ \Sigma = \sum_{i=1}^{D}\lambda_{i}\mu_{i}\mu_{i}^{T} $$

把上式带入二次型可得：

$$ \Delta^{2}  = \sum_{i=1}^{D}\frac{y_{i}^{2}}{\lambda_{i}} $$

其中 $y_{i} = \mu_{i}^{T}(x-\mu) $

二次型在上式为常数的曲面上为常数，因此高斯密度也是常数。如果所有的特征$\lambda_{i}$都是正数，那么这些曲面表示椭球面(球心位于$\mu$，椭球的轴的方向沿着$\mu_{i}$，沿着轴向的缩放因子为$\lambda_{i}^{\frac{1}{2}}$)。

对于要定义的高斯分布，有必要要求协方差矩阵的所有特征值$\lambda_{i}$严格大于零，否则分布将不能被正确的归一化。一个特征值严格大于零的矩阵被成为正定矩阵。在后面，我们会看到，我们会遇到一个多个特征值为零的高斯分布，那种情况下分布是奇异的，被限制在一个低维的子空间中。如果所有的特征值都是非负的，那么这个矩阵被称为半正定矩阵。

接下来考虑由$y_{i}$定义的新坐标系下高斯分布的形式。从x坐标到y坐标系，我们由一个Jacobian
矩阵J，它的元素为：

$$ J_{ij} = \frac{\partial x_{i}}{\partial y_{j}} = U_{ij} $$

那么在$y_{i}$坐标系中，高斯分布的形式为：

$$ p(y) = p(x)|J| = \prod_{j=1}^{D}\frac{1}{(2\pi\lambda_{j})^{\frac{1}{2}}}exp\{-\frac{y_{j}^{2}}{2\lambda_{j}}\} $$

高斯分布的矩，描述了参数$\mu$和$\sigma$：

$$ E[x] = \mu $$

$$ E[xx^{T}] = \mu\mu^{T} + \Sigma $$

$$ var[x] = \Sigma $$

虽然高斯分布被广泛用作概率密度模型，但一方面它的参数数量巨大，会随着维度D以平方的方式在增长，并且对大矩阵求逆会变得无法计算。本另一方面高斯分布本质上还是单峰的，对于一些分布无法近似，这可以通过引入潜在变量来解决。

### 2.3.1 条件高斯分布

多元高斯分布的一个重要性质是，如果两组变量是联合高斯分布，那么以一组变量为条件，另一组变量同样是高斯分布。类似地，任何一个变量的边缘分布也是高斯分布。下面我们对这两个性质进行简要证明。该证明用到了二次型的性质,而后在计算的最后阶段重新考虑归一化系数。

首先我们假设$x_{a}$和$x_{b}$是一个高斯分布中的两个相互独立的部分，这样它们的联合分布就是高斯分布。我们在本部分**先求条件概率分布
$p(x_{a} | x_{b})$表达式**。

因此：

$$
x = \left(
    \begin{aligned}
    x_{a} \\
    x_{b} 
    \end{aligned}
    \right)
$$

$$
\mu = \left(
    \begin{aligned}
    \mu_{a} \\
    \mu_{b} 
    \end{aligned}
    \right)
$$


$$
\Sigma = \left(
    \begin{aligned}
    \Sigma_{aa} ~&~ \Sigma_{ab}\\
    \Sigma_{ba} ~&~ \Sigma_{bb}
    \end{aligned}
    \right)
$$

精度矩阵$\Lambda$：

$$
\Lambda = \left(
    \begin{aligned}
    \Lambda_{aa} ~&~ \Lambda_{ab}\\
    \Lambda_{ba} ~&~ \Lambda_{bb}
    \end{aligned}
    \right)
$$

这样我们可以对联合高斯分布的二次型进行展开：

$$
-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu) = 
-\frac{1}{2}(x_{a} - \mu_{a})^{T}\Lambda_{aa}(x_{a} - \mu_{a}) - \frac{1}{2}(x_{a} - \mu_{a})^{T}\Lambda_{ab}(x_{b} - \mu_{b}) \\
-\frac{1}{2}(x_{b} - \mu_{b})^{T}\Lambda_{ba}(x_{a} - \mu_{a}) - \frac{1}{2}(x_{b} - \mu_{b})^{T}\Lambda_{bb}(x_{b} - \mu_{b})
$$

我们把它看成是$x_{a}$的函数，这又是一个二次型。我们注意到一个一般的高斯分布
$N(x | \mu, \Sigma)$的指数可以写成：

$$ -\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu) = -\frac{1}{2}x^{T}\Sigma^{-1}x + x^{T}\Sigma^{-1}\mu + const $$

其中常数表示与x无关的项，因此通过与一般的高斯分布进行比对，我们可以发现所有x的二阶项可以比对出方差：

$$ \Sigma_{a|b} = \Lambda_{aa}^{-1} $$

对比所有x的常数项，可以得到均值：

$$ \mu_{a|b} = \Sigma_{a|b}(\Lambda_{aa}\mu_{a} - \Lambda_{ab}(x_{b} - \mu_{b}))\\
        = \mu_{a} - \Lambda_{aa}^{-1}\Lambda_{ab}(x_{b} - \mu_{b}) $$

我们也可以通过协方差矩阵表达上式，原理类似，这里不再赘述。

### 2.3.2 边缘高斯分布

上面我们已经看到，如果联合分布是高斯分布，那么条件概率也是高斯分布。现在我们要讨论边缘概率分布：

$$ p(x_{a}) = \int p(x_{a}, x_{b})dx_{b} $$

我们接下来证明这也是一个高斯分布，和之前一样，我们把注意力集中于联合分布的指数项的二次型，然后找出边缘分布$p(x_{a})$的均值和协方差。

由于流程类似，这里直接给出由二次型给出的结果：

$$ \Sigma_{a} = (\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1} $$

$$ \mu_{a} = \Sigma_{a}(\Lambda_{aa} - \Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})\mu_{a} $$

当我们使用协方差矩阵表示均值和方差时公式将得到简化，这与条件概率分布那里不同：

$$ E[a_{a}] = \mu_{a} $$

$$ cov[x_{a}] = \Sigma_{aa} $$

### 2.3.3 高斯变量的贝叶斯定理

本小节我们希望在假定给出一个高斯边缘分布p(x)和一个高斯条件分布
$p(y|x)$，其中
$p(y|x)$是x的线性函数，与协方差x无关(线性高斯模型)，找到边缘概率分布$p(y)$和条件概率分布
$p(x | y) $。

我们令边缘概率分布和条件概率分布的形式如下：

$$ p(x) = N(x | \mu, \Lambda^{-1}) $$

$$ p(y | x) = N(y | Ax + b, L^{-1}) $$

其中$\mu$，A和b是控制均值的参数，$\Lambda$和L是精度矩阵。

我们首先求联合概率分布的表达式。因此联合概率分布可以表示为边缘概率分布和条件概率分布的乘积，因此对其取对数有：

$$ 
\begin{aligned}
ln p(z) & = ln p(x) + ln p(y | x) \\
        & = -\frac{1}{2}(x-\mu)^{T}\Lambda(x-\mu) - \frac{1}{2}(y-Ax-b)^{T}L(y-Ax-b) + Const 
\end{aligned}
$$

通过对上式变形并比对二次型，我们可以得到联合分布的均值和协方差：

$$ cov[z] = R^{-1} = \left(
    \begin{aligned}
    \Lambda^{-1} ~&~ \Lambda^{-1}A^{T} \\
    A\Lambda^{-1} ~&~ L^{-1} + A\Lambda^{_1}A^{T}
    \end{aligned}
    \right)
$$

$$ E[z] = \left(
    \begin{aligned}
    \mu \\
    A\mu + b
    \end{aligned}
    \right)
$$

接下来可以求边缘分布$p(y)$的表达式，这个边缘分布是通过对x求积分得到：

$$ E[y] = A\mu + b $$

$$ cov[y] = L^{-1} + A\Lambda^{-1}A^{T} $$

对于条件分布，我们可以通过类似的方法的到，对应的均值和方差为：

$$ E[x | y] = (\Lambda + A^{T}LA)^{-1}\{A^{T}L(y-b) + a\mu \} $$

$$ cov[x | y] = (\Lambda + A^{T}LA)^{-1} $$

### 2.3.5 顺序估计

顺序估计的方法允许每次处理一个数据点，然后丢弃这个点 

现在考虑搞死最大似然估计的估计结果$\mu_{ML}$。当它依赖于第N次观察是，将被记做$\mu_{ML}^{(N)}$。如果我们想分析最后一个数据点$x_{N}$的贡献，则：

$$
\begin{aligned}
\mu_{ML}^{(N)}  & = \frac{1}{N}\sum_{n=1}^{N}x_{n} \\
  & = \mu_{ML}^{(N-1)} + \frac{1}{N}(x_{N} - \mu_{ML}^{(N-1)}) 
\end{aligned}
$$

可以看到，在观察到N-1个数据点后，我们已经把$\mu$估计为$\mu_{ML}^{N-1}。我们现在获取了数据点$x_{N}$，这样我们就得到了一个修正的估计$\mu_{ML}^{N}$，这个估计的获得方式为：把旧的估计沿着“错误信号”$(x_{N} - \mu_{ML}^{(N-1)})方向移动一个微小的量，这个量正比于$\frac{1}{N}$。注意，随着N的增加，后续数据点的贡献逐步减小。

我们不能总是通过使用上述方法推出一个顺序算法，一个更加通用的算法是Robbins-Monro算法。

老=考虑一对随机变量$\theta$和z，它们由一个联合概率分布$p(z, \theta)$所控制。已知$\theta$的情况下，z的条件期望定义了一个确定的函数$f(\theta)$，形式如下：

$$ f(\theta) = E[z | \theta] = \int zp(z | \theta) dz $$

通过这种方式定义的函数被称为回归函数。我们的目标是寻找根$\theta^{\*}$使得$f(\theta^{\*})=0$。那么估计$\theta^{\*}$的顺序方法由Robbins-Monro算法给出。

我们假定z的条件方差是有穷的，因此：

$$ E[(z-f)^{2}][\theta] < \infty $$

并且不失一般性，我们也假定当$\theta>\theta^{\*}$时$f(\theta) > 0$，当$\theta<\theta^{\*}$时，$f(\theta) < 0$。之后，Robbins-Monro方法定义了一个根$\theta^{\*}$的顺序及的序列：

$$ \theta^{(N)} = \theta^{(N-1)} - \alpha_{N-1}z(\theta^{(N-1)}) $$

### 2.3.6 高斯分布的贝叶斯推断

最大似然框架给出了对于参数$\mu$和$\Sigma$的点估计，现在通过引入这些参数的先验分布，介绍一种贝叶斯方法。

#### 2.3.6.1 方差$\sigma^{2}$已知，推断$\mu$

先把似然函数写出来，它是$\mu$的函数：

$$ p(x | \mu) = \prod_{n=1}^{N}p(x_{n} | \mu) = \frac{1}{2\pi\sigma^{2}}exp\left\{-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(x_{n} - \mu^{2})\right\} $$

我们看到似然函数的形式为$\mu$的二次型的指数形式。因此我们可以选取先验分布$p(\mu)$为高斯分布，那么它就是似然函数的一个共轭分布，对应的后验概率是两个$\mu$二次函数的指数的乘积，因此也是一个高斯分布。因此：

$$ p(\mu) = N(\mu | \mu_{0},\sigma_{0}^{2}) $$

$$ p(\mu | x) \propto p(x | \mu)p(\mu) $$

对应的后验概率形式为：

$$ p(\mu | x) = N(\mu | \mu_{N}, \sigma_{N}^{2}) $$

其中：

$$ \mu_{N} = \frac{\sigma^{2}}{N\sigma_{0}^{2} + \sigma^{2}}\mu_{0} + \frac{N\sigma_{0}^{2}}{N\sigma_{0}^{2} + \sigma^{2}}\mu_{ML} $$

$$ \frac{1}{\sigma_{N}^{2}} = \frac{1}{\sigma_{0}^{2}} + \frac{N}{\sigma^{2}} $$

其中$\mu_{ML}$是$\mu$的最大似然解，由样本均值给出：

$$ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} $$

由上面的公式我们可以得出如下结论：

* 后验分布的均值$\mu_{N}$是鲜艳均值$\mu_{0}$和最大似然解$\mu_{ML}$的折中。当N=0时，$\mu_{N}$就变成了先验均值。当$N \rightarrow \infty$时，后验均值由最大似然解给出。    
* 对于精度来说，我们看到精度是可以相加的，因此对于后验概率的精度等于先验的精度加上每一个观测数据点所贡献的一个精度。当我们增加数据观测点的数量时，精度持续增加，而对应于后验分布的方差持续减少。    

而对于推断问题，从顺序的观点来看，贝叶斯方法将变得十分自然。

$$ p(\mu | x) \propto \left[ p(\mu)\prod_{n=1}^{N}p(x_{n} | \mu) \right] p(x_{N} | \mu) $$

方括号中的项是观测到N-1个数据点之后的后验概率分布。我们可以将它看做一个先验分布，然后使用贝叶斯定理与似然函数结合到了一起。这种贝叶斯推断的顺序观点是十分通用的，可以应用于任何观测数据独立同分布的问题中。

#### 2.3.6.2 $\mu$已知，求方差

和前面一样，先写出精度(方差的倒数)的似然函数形式：

$$ p(x | \lambda) = \prod_{n=1}^{N}N(x_{n} | \mu, \lambda^{-1}) \propto \lambda^{\frac{1}{2}}exp\left\{ -\frac{\lambda}{2}\sum_{n=1}^{N}(x_{n} - \mu)^{2} \right\} $$

因此，对应的共轭先验应该正比于$\lambda$的幂指数，也正比于$\lambda$的线性函数的指数。这对应于Gamma分布，定义为：

$$ Gam(\lambda | a, ,b) = \frac{1}{\Gamma(a)}b^{a}\lambda^{(a-1)}exp(-b\lambda) $$

Gam分布对应的均值和方差为：

$$ E[\lambda] = \frac{a}{b} $$

$$ var[\lambda] = \frac{a}{b^{2}} $$

将Gam先验分布乘以似然函数得到后验分布：

$$ p(\lambda | x) \propto \lambda^{a_{0} - 1}\lambda^{\frac{N}{2}}exp\left\{ -b_{0}\lambda - \frac{\lambda}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\} $$

可以把它看做形式为
$Gam(\lambda | a_{N}, b_{N})$ 的Gamma分布，其中：

$$ a_{N} = a_{0} + \frac{N}{2} $$

$$ b_{N} = b_{0} + \frac{1}{2}\sum_{n=1}^{N}(x_{n} - \mu)^{2} = b_{0} + \frac{N}{2}\sigma_{ML}^{2} $$

可以看到，N个数据点的观测效果是把系数a的值增加$\frac{N}{2}$。类似地，对于参数b贡献了$\frac{N\sigma_{ML}^{2}}{2}$。

#### 2.3.6.3 当方差和均值都未知时

通过观察$\mu$，$\lambda$的先验分布，我们得出归一化的先验概率的形式为：

$$ p(\mu, \lambda) = N(\mu | \mu_{0}, (\beta\lambda)^{-1})Gam(\lambda | a, b) $$

其中：

$$ \mu_{0} = \frac{c}{\beta} $$

$$ a = \frac{1+\beta}{2} $$

$$ b = d - \frac{c^{2}}{2\beta} $$

上述分布被成为正态-Gamma分布。

### 2.3.7 学生t分布

我们已经看到高斯分布的精度的共轭先验是Gamma分布。如果我们有一个一元高斯分布
$N(x | \mu, \tau^{-1})$和一个Gamma先验
$Gam(\tau | a, b)$，我们把精度积分出来，可以得到x的边缘分布：

$$ p(x | \mu, a, b) = \int_{0}^{\infty} N(x | \mu, \tau^{-1})Gam(\tau | a, b)d\tau $$

我们定义新的参数$\nu=2a$ 和 $\lambda = \frac{a}{b}$。使用新的参数对其进行替换有：

$$ St(x | \mu,\lambda, \nu) = \frac{\Gamma(\frac{\nu}{2} + \frac{1}{2})}{\Gamma(\frac{\nu}{2})}\left( \frac{\lambda}{\pi\nu}  \right)^{\frac{1}{2}}\left[ 1 + \frac{\lambda(x-\mu)^{2}}{\nu} \right]^{-\frac{\nu}{2}-\frac{1}{2}} $$

根据似然函数的积分公式可以见到，学生t分布可以通过无限多个同均值不同精度的高斯分布相加的方式得到。这可以表示为无线的高斯混合模型。结果是一个概率分布，这个分布有着比高斯分布更长的尾巴。这给出了t分布的一个重要性质-鲁棒性，意思是对于数据集中的几个离群点的出现，t分布不会像高斯分布那样敏感。

### 2.3.9 高斯混合模型

通过将更基本的概率分布(如高斯分布)进行线性组合的这样的叠加方法，可以被形式化为概率模型，被称为混合模型。通过足够多的高斯分布，并调节它们的均值和方差以及线性组合的系数，几乎所有的连续概率密度都能够以任意的精度近似。

于是我们考虑K个高斯概率密度的叠加，形式为：

$$ p(x) = \sum_{k=1}^{K}\pi_{k}N(x | \mu_{k}, \Sigma_{k}) $$

这被称为混合高斯。每一个高斯概率密度，每一个高斯概率密度
$N(x | \mu_{k}, \Sigma_{k})$被称为混合分布的一个成分，并且有自己的均值和方差。$\pi_{k}$被称为混合系数。

## 2.4 指数族分布

前面我们研究的概率分布(高斯混合分布除外)都是一大类被称为指数族分布的概率分布的特例。

参数为$\eta$的变量x的指数族分布定义为具有下面形式的概率分布的集合：

$$ p(x | \eta) = h(x)g(\eta)exp\{\eta^{T}u(x)\} $$

其中x可能是标量或者向量，可能是离散的或连续的。这里$\eta$被称为概率分布的自然参数，$u(x)$是x的某个函数。函数$g(\eta)$可以被看成系数，它确保了概率分布是归一化的，因此满足：

$$ g(\eta)\int h(x)exp\{\eta^{T}u(x)\} dx = 1 $$

### 2.4.1 最大似然与充分统计量

具体方法还是求梯度并令梯度为0得到最大似然解。

### 2.4.2 共轭先验

对指数族分布的任何成员，都存在一个共轭先验，可以写成下面的形式：

$$ p(\eta | \chi, \nu) = f(\chi, \nu)g(\eta)^{\nu}exp{\nu\eta^{T}\chi} $$

其中$f(\chi, \nu)$是归一化系数，$g(\eta)$与之前含义相同。

### 2.4.3 无信息先验

无信息先验是指先验分布对尽量对后验分布产生尽可能小的影响。较为简单的无信息先验如：

$$ p(x| \mu) = f(x-\mu)$$

那么参数$\mu$被称为位置参数。这一类概率分布具有平移不变性。另一个较为简单的无信息先验分布如：

$$ p(x | \sigma) = \frac{1}{\sigma}f(\frac{x}{\sigma}) $$

其中$\sigma > 0$，被称为缩放参数，概率密度具有缩放不变形。

## 2.5 非参数化方法

在前面我们已经关注过的概率分布都有具体的函数形式，并且由少量的参数控制。这些参数的额值可以由数据确定，这被称为概率密度的参数化方法。与之相反，还存在一些非参数化的方法，这种方法对概率分布的形式进行很少的假设。最常用、简单的三种非参数化方法有：直方图、核密度估计、近邻方法等。

直方图法就是我们常见的通过化直方图的方式进行建模，直方图箱子的宽度决定了模型的平滑程度。

核密度估计是假设观测服从D维空间的某个位置的概率密度p(x)。把该空间划分为多个很小的区域并假定在该区域内的数据点服从一定的分布，如二项分布或高斯分布等，这也就对应了不同的核函数。

近邻方法就是机器学习里那个最近邻法，K最近邻就是KNN，这里不再赘述。
