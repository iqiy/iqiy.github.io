---
layout:     post
title:      "图像处理基础（三）"
subtitle:   "SFM和SLAM"
date:       2020-06-11 23:20:00
author:     "QIY"
header-img: "img/IAP.png"
header-mask: 0.3 
catalog:    true
tags:
    - image processing 
---

> 图像处理 的学习基本上是学十得一。稳扎稳打再来一次

* TOC

{:toc}

三维重建基础SFM和SLAM

# 1 背景
对比“图像处理基础（二）中描述了单张图片的标定”本文为摄像机自标定方法。
1.  对极几何概念
![](/img/in-post/200611_SFM-SLAM/80e39bfca90dd30b2530e5081b8d8f85.png)
概念：
1.  基线：连接两个摄像机光心的直线C0C1；
2.  （外）极点：基线与像平面的交点e0，e1；
3.  （外）极平面：过基线和p点的平面c0-c1-p;
4.  (外)极线：对极平面和图像平面的交线e0-x;
5.  基本矩阵F：对应点对之间的约束；
通过几何关系可求出本质矩阵E，进而估算出外部参数R，t，进而达到三维重建的目的；
计算步骤忽略；
# 2 SFM
## 2.1 概念
Structure from motion 指的是由图像生成3维点云以及相机姿态，即：
\- 输入：一系列图像，拍摄同一场景 ；
\- 输出： 每一张图像对应的相机位置和朝向， 场景中的3D点云；
![](/img/in-post/200611_SFM-SLAM/680bdba45f96c3c7992bd2b353d48d3e.png)
## 2.2 流程简介
### 2.2.1 特征点提取与特征点匹配
特征点提取：Shi&Tomasi、SIFT、SURF
特征点匹配：描述子计算：匹配结果往往有很多误匹配，为了排除这些错误，使用KNN算法寻找与该特征最匹配的2个特征，若第一个特征的匹配距离与第二个特征的匹配距离之比小于某一阈值，就接受该匹配，否则视为误匹配。当然，也可以使用交叉验证方法来排除错误。
### 2.2.2 基础矩阵估计F
基础矩阵中有9 个元素
5点法：略
8点法：采用了RANSAC的方法进行对E进行估计，每一步迭代的过程中，利用8点法进行求解。
### 2.2.3 本质矩阵估计E
本征矩阵有7个独立参数，估计出本质矩阵的目的是为了对之前求得的匹配进行约束，得到的匹配成为几何一致匹配，不同图像上的几何一致匹配形成了一个TRACK（其实就是一个空间点在不同的图像上的投影点之间的匹配）。
### 2.2.4 本质矩阵分解为R和T
-   SVD分解
-   存在4种可能的解，寻找正确的解
-   检查旋转矩阵R的正确性
-   R的行列式必须为1或者-1
### 2.2.5 三维点云计算
三角形法
-   已经知道了两个相机之间的变换矩阵(R和T)，还有每一对匹配点的坐标，通过这些已知信息还原匹配点在空间当中的坐标，根据公式：
![](/img/in-post/200611_SFM-SLAM/3d9a9728f8147d619f57d9b9f5b5c615.png)
这个等式中有两个未知量，分别是s2,x 。用s2对等式两边做叉积，可以消去s2，得：
![](/img/in-post/200611_SFM-SLAM/ebcea93941f8087f1dc7c50efdc4a27a.png)

用SVD求X左边矩阵的零空间，再将最后一个元素归一化到1，即可求得X。其几何意义相当于分别从两个相机的光心作过x1和x2的延长线，延长线的焦点即为方程的解，如文章最上方的图所示。

### 2.2.6 重投影

将三维点三角化并重映射到摄像机得到二维点，计算与最初二维点之间的距离，说明三角化误差。

### 2.2.7 计算第三个摄像机到到世界坐标系的变换矩阵(R和T)

假设：用于多目重建的图像是有序的，即相邻图像的拍摄位置也是相邻的。

猜想：

-   最简单的想法，就是沿用双目重建的方法，即在第三幅图像和第一幅图像之间提取特征点，然后估计本征矩阵E。那么加入第四幅、第五幅，乃至更多呢？随着图像数量的增加，新加入的图像与第一幅图像的差异可能越来越大，特征点的提取变得异常困难，这时就不能再沿用双目重建的方法了。

-   用新加入的图像和相邻图像进行特征匹配，然后计算E，但这是计算的是相对变换，比如相机三到相机二的变换，而我们需要的是相机三到相机一的变换。有人说，既然知道相机二到相机一的变换，又知道相机到三到相机二的变换，不就能求出相机三到相机一的变换吗？实际上，通过这种方式，你只能求出相机三到相机一的旋转变换（旋转矩阵R），而他们之间的位移向量T，是无法求出的。这是因为上面两个函数求出的位移向量，都是单位向量，丢失了相机之间位移的比例关系。

算法描述：

-   摄像机标定或摄像机之态估计，对于输入的第三幅图片，计算第三幅图片与第二幅图片的匹配点，这些匹配点中，肯定有一部分也是图像二与图像一之间的匹配点，也就是说，这些匹配点中有一部分的空间坐标是已知的，同时又知道这些点在第三幅图像中的像素坐标，即可计算变换矩阵。

>   透视N点法（PNP）

-   三角化更多的点并查看这些点是如何融入存在的几何结构中，然后进行求解。

>   迭代最近点法（ICP）

### 2.2.8 更多摄像相机的变换矩阵计算

得到第三个摄像机的变换矩阵后，就可以计算匹配点的在空间中的坐标，得到三维点云，将新得到的三维点云与之前计算得三维点云进行融合（已经存在的空间点，就没必要再添加了，只添加在图像二和三之间匹配，但在图像一和图像三中没有匹配的点）。然后循环迭代，如下图所示。

![](/img/in-post/200611_SFM-SLAM/4c3b2efa331c8da745c3a1d1e36d6e62.jpg)

### 2.2.9 重构的细化与优化

-   原因：随着图像的不断增加，误差会不断累积，最后误差过大以至于完全偏离重建的目标。

-   目的：三维点云的位置和摄像机的位置优化

-   算法：
>   光束法平差（Bundle Adjustment）
>   BA本质上是一个非线性优化算法
>   **简单稀疏光束调整**（SSBA）
>   Ceres Solver
Google的一个开源项目，用来求解非线性最小二次问题的库，因此可以用来求解BA。
>   Ceres
>   Solver专为求解此类问题进行了大量的优化，有很高的效率，尤其在大规模问题上，其优势更加明显。
1.  OpenCV3中的SFM
主要包括两种重建的接口cv::sfm::reconstruct，一种是输入图像序列，一种是输入跟踪点序列，计算出相机在两帧空间场景间的R和T，算法具体实现基于libmv库。
[](https://developer.blender.org/project/profile/59/)
libmv是Blender旗下的一个开源项目。libmv是一个通过运动计算结构的库。
# 3 SLAM
## 3.1 SLAM模型简况
流程：
![](/img/in-post/200611_SFM-SLAM/20200611235045.png)
方法:
![](/img/in-post/200611_SFM-SLAM/412f23a54341efc8a9c3ff6aafe4a73d.png)
视觉SLAM包括几个流程，前端视觉里程计，后端优化，回环检测以及建图。每个part又可以用不同的方法实现。
细节：
![](/img/in-post/200611_SFM-SLAM/01f2cf43416b5c42fdfd6d61ef8bd3fa.png)
1. 传感器信息读取。在视觉 SLAM
中主要为相机图像信息的读取和预处理。如果在机器人中，还可能有码盘、惯性传感器等信息的读取和同步。
2. 视觉里程计 (Visual Odometry, VO)。视觉里程计任务是估算相邻图像间相机的运动，
以及局部地图的样子。VO 又称为前端（Front
End）。简而言之，第一是要根据摄像头回传的图像计算相机帧间运动的旋转矩阵R和平移向量t（相机是怎么运动的）；第二就是估计路标点大致的空间位置坐标（深度信息）。
3.
后端优化（Optimization）。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行优化，得到全局一致的轨迹和地图。由于接在
VO 之后， 又称为后端（Back End）。
4. 回环检测（Loop
Closing）。回环检测判断机器人是否曾经到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。当摄像头在环境中到达了自己曾经先前到达过的位置后，因为传感器和计算过程存在误差，所以通常建立出的点云图都不能做到曲线闭合。回环检测的功能便是判断相机是否回到过先前位置，从而修正误差，保证点云图与实际空间的理想。
5. 建图（Mapping）。它根据估计的轨迹，建立与任务要求对应的地图。

## 3.2 名词
基础知识点    
    1. 特征提取、特征匹配
    （1）Harris
    （2）SIFT
    （3）SUFT
    （4）ORB
    （5）特征匹配
    2. 2D-2D：对极约束、基础矩阵、本质矩阵、单应矩阵
    3. 3D-2D:PnP  
    （1）直接线性变换方法
    （2）P3P方法  
    （3）Bundle Adjustment方法  
    4. 3D-3D:ICP 
    （1）SVD方法
    （2）非线性优化方法
    5. 直接法和光流法
    （1）光流法
    （2）直接法
    6. Bundle Adjustment
    回环检测
    相关问题
    1. SIFT和SUFT的区别
    2. 相似变换、仿射变换、射影变换的区别
    3. Homography、Essential和Fundamental Matrix的区别
    4. 视差与深度的关系
    5. 描述PnP算法
    6. 闭环检测常用方法
    7. 给一个二值图，求最大连通域
    8. 梯度下降法、牛顿法、高斯-牛顿法的区别
    9. 推导一下卡尔曼滤波、描述下例子滤波
    10. 如何求解\$Ax=b\$的问题
    11. 什么是极限约束
    12. 单目视觉SLAM中尺寸漂移是怎么产生的
    10. 解释SLAM中的绑架问题
    11. 描述特征点法和直接法的优缺点
    12. EKF和BA的区别
    13. 边缘检测算子有哪些？
    14. 简单实现cv::Mat()
    15. 10个相机同时看到100个路标点，问BA优化的雅克比矩阵多少维
    16. 介绍经典的视觉SLAM框架
    （<https://blog.csdn.net/weixin_44580210/article/details/87214464>）

# 4 SFM与SLAM区别与联系

区别（1）SLAM要求实时，数据是线性有序的，无法一次获得所有图像，部分SLAM算法会丢失过去的部分信息；基于图像的SfM不要求实时，数据是无序的，可以一次输入所有图像，利用所有信息。（2）SLAM是个动态问题，会涉及到滤波，运动学相关的知识，而SfM主要涉及的还是图像处理的知识。

联系（1）基本理论是一致的，都是多视角几何；（2）传统方法都需要做特征值提取与匹配；（3）都需要优化投影误差；（4）回环矫正和SfM的全局注册方法是同一件事情。
SFM参考：
<https://www.cnblogs.com/clarenceliang/p/6704970.html>
<https://www.zhihu.com/question/64011093>
<https://blog.csdn.net/weixin_44580210/article/details/87214464>
<https://blog.csdn.net/frozenspring/article/details/86155501>
SLAM参考：
<https://yq.aliyun.com/articles/79457?utm_campaign=wenzhang&utm_medium=article&utm_source=QQ-qun&2017511&utm_content=m_20422>
<https://blog.csdn.net/m0_37702666/article/details/79402267>
<https://www.cnblogs.com/2008nmj/p/6344828.html>
<https://blog.csdn.net/qq_37427972/article/details/83033067?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-2>
<https://blog.csdn.net/weixin_44580210/article/details/87214464> 典
