---
layout: post
title: "DL学习笔记（五）"
subtitle: "GNN 图神经网络"
date: 2020-06-09 12:20:18
author: "QIY"
header-img: "img/DL.png"
header-mask: 0.3
catalog: true
tags:
    - Deep Learning
---


> Deep Learning 的学习需稳扎稳打 积累自己的小厂库

* TOC
{:toc}

# 1 图（数据结构课程）
**G=(V,E,A)**<br />
1）V(N个)是节点集合, 只描述了存在的节点<br />
2）E是边集合, 只描述了存在的连接(边)<br />
3）A(NxN)是图的邻接矩阵, 描述了节点之间的连接关系,
当对应的边eij(连接vi,vj的边)存在的时候, 则对应的Aij = wij\> 0, 若eij不存在,则Aij = 0<br />
4）D(NxN)是图的度矩阵, 描述了节点自身连接的边的数目, 是对角矩阵,Dii=(∑Ai)D_{ii}= ( \\sum
A_i)*Dii*​=(∑*Ai*​)（[度矩阵和邻接矩阵的关系可以参考这里](https://blog.csdn.net/Frank_LJiang/article/details/95214027)）<br />
5）X(NxF)是节点属性矩阵,每个节点对应拥有一个长为F的属性向量, L=D−A,Ln=In−D−1/2AD−1/2L=D-A,L_n=I_n-D\^{-1/2}AD\^{-1/2}*L*=*D*−*A*,*Ln*​=*In*​−*D*−1/2*AD*−1/2,
图可以用节点属性来关联。<br />
6）L(NxN)是图的拉普拉斯矩阵,归一化的拉普拉斯矩阵是图的一个鲁棒的数学表示.归一化图拉普拉斯矩阵具有实对称半正定的性质,也就是可以进行特征分解得到特征值和特征向量。<br />
# 2 图神经网络
## 2.1 起始
The graph neural network model (Scarselli, F., et al., 2009) [1]
这篇论文中的算法，这是第一次提出 GNN 的论文，因此通常被认为是原始
GNN，目前主要使用图卷积网络GCN。
## 2.2 输入输出
输入：使用图结构和节点内容信息作为输入；
输出：
GCN的输出使用以下不同的机制, 根据不同的分析任务而定：<br />
1）节点级输出与节点回归和分类任务相关. 图卷积模块直接给出节点潜在的表达,多层感知机或者softmax层被用作最终的GCN层.<br />
2）边级输出与边分类和连接预测任务相关. 为了预测边的标记和连接强度,一个附加函数将会把来自图卷积模块的两个节点的潜在表达作为输入.<br />
3）图级输出与图分类任务有关. 为了获取一个图级的紧凑表达,池化模块被用来粗化图到子图, 或用来加和/平均节点表达。<br />
## 2.3 方法
基于节点的局部邻域信息对节点进行embedding。直观来讲，就是通过神经网络来聚合每个节点及其周围节点的信息。（将图中的节点视为目标或者concept，边表示顶点间关系。每个concept
都自然的通过各自的特征和关联concepts的特征定义。然后通过顶点包含的信息以及其邻域的信息，给每个顶点一个状态向量）
![](/img/in-post/200609_GNN/63f9dfb95bdce67e0eaeb964c8158ac3.png)
节点如何获取它的邻居节点的信息。最基本的想法就是聚合一个节点的邻居节点信息时，采用平均的方法，并使用神经网络作聚合操作，具体方法如下图：
![](/img/in-post/200609_GNN/6567c247fdcb809e00d787686a4a3c59.png)
那么如何训练这个模型，具体分为监督、和无监督两种方法。
无监督的方法包括：
随机游走（Random walks）：node2vec， DeepWalk
图分解（Graph factorization）
训练模型使得相似的节点具有相似的embedding
监督的方法，以二分类举例，可以定义一个交叉熵函数来作为损失函数（类似svm的损失）：
![](/img/in-post/200609_GNN/7552b140229686d1faafeb152e335969.png)
图神经网络中每一层的聚合所使用的参数是相同的，并且这个模型可以推理出新出现节点的embedding或者一张新图的embedding。
参考:
<https://blog.csdn.net/Frank_LJiang/article/details/95194733>

<https://blog.csdn.net/m0_43421755/article/details/103181937>
