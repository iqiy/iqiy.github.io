---
layout: post
title: "DL学习笔记（一）"
subtitle: "BP神经网络白菜入门题"
date: 2020-06-08 22:20:18
author: "QIY"
header-img: "img/DL.png"
header-mask: 0.3
catalog: true
tags:
    - Deep Learning
---


> Deep Learning 的学习需稳扎稳打 积累自己的小厂库

* TOC
{:toc}

# 1 概要

对于BP神经网络<br />
1个闭环，2个过程：“前向传播求损失” “反向传播求梯度”<br />
4个名词：<br />
梯度函数<br />
损失函数<br />
权重<br />
激励函数（sigmoid）<br />
![](/img/in-post/200608_BP/c2f434344644e5abc478244fa8edadc1.png)
# 2 题干
![](/img/in-post/200608_BP/5cb3a589bdeaa8acaafa3c16bb7ff3af.png)
已知节点1输入0.35,节点2输入0.9，节点5输出0.5，求各节点间合适的权重值。
解：
\*其中对应的矩阵表示如下
![](/img/in-post/200608_BP/4ef5c9ecd0bad7083d9a4c109117086c.jpg)
## 2.1 正向传播求损失
首先我们先走一遍正向传播，公式与相应的数据对应如下：
![](/img/in-post/200608_BP/3dfc99a7a4ef297a593a740d8c93a486.png)
那么：
![](/img/in-post/200608_BP/350a961eba6872a6fdf9d25d5cd6c64a.png)
同理可以得到：
![](/img/in-post/200608_BP/b2d73b10501116e86707fbde95638af0.png)
那么最终的损失为
![](/img/in-post/200608_BP/a83dfaaac5d410774fad5b3793c566b3.jpg)
## 2.2 反向传播求梯度
我们当然是希望这个值越小越好。这也是我们为什么要进行训练，调节参数，使得最终的损失最小。这就用到了我们的反向传播算法，实际上反向传播就是梯度下降法中链式法则的使用。
下面我们看如何反向传播
根据公式，我们有：
![](/img/in-post/200608_BP/09630b20d5fabc1983707dff1781f059.png)
这个时候我们需要求出**C对w的偏导**，则根据链式法则有：
![](/img/in-post/200608_BP/d87a13085e8fa2d4302d47bb1cefb391.png)
同理有：
![](/img/in-post/200608_BP/9c82d0a2435b916e011f23f3869a1a2f.png)
到此我们已经算出了最后一层的参数偏导了.我们继续往前面链式推导：
我们现在还需要求
![](/img/in-post/200608_BP/54eb2c4b6348ebca2e055d71b90cd25b.jpg)
\*下面给出一个推导其它全都类似
![](/img/in-post/200608_BP/b7fa6c26864b7d4228a34a57b890fb62.png)
同理可得其它几个式子：
则最终的结果为：
![](/img/in-post/200608_BP/5a62b499d7173bb4e0355bd43b6b3a1c.png)
## 2.3 迭代
再按照这个权重参数进行一遍正向传播得出来的Error为0.165。<br />
而这个值比原来的0.19要小，则继续迭代，不断修正权值，使得代价函数越来越小，预测值不断逼近0.5，这次迭代了100次的结果，Error为5.92944818e-07（已经很小了，说明预测值与真实值非常接近了），最后的权值为：
![](/img/in-post/200608_BP/f498d477793f0c82cd6da7dc605e9e68.png)
bp过程可能差不多就是这样了。
