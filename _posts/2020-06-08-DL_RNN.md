---
layout: post
title: "DL学习笔记（三）"
subtitle: "RNN 循环神经网络"
date: 2020-06-08 22:20:18
author: "QIY"
header-img: "img/DL.png"
header-mask: 0.3
catalog: true
tags:
    - Deep Learning
---


> Deep Learning 的学习需稳扎稳打 积累自己的小厂库

* TOC

{:toc}

# 1 RNN基本架构
与cnn相比多了 存储，输出结果是输入加store的累加；
![](/img/in-post/200608_RNN/2aab278f6f706c0ba76bc219e9a74cac.png)
![](/img/in-post/200608_RNN/6c9007f35e068c74b30c388c30aa703d.png)
![](/img/in-post/200608_RNN/40613eaa5b77c0d54ae7a9ef0caca187.png)
![](/img/in-post/200608_RNN/7b4b0b21985a9fe90b11318d47126357.png)
![](/img/in-post/200608_RNN/561605d4437ce89976f9855dca5e9e73.png)
![](/img/in-post/200608_RNN/a1652bab581b86fecc07dcda4518bc2f.png)
\--

# 2 LSTM
说道RNN一般指的就是LSTM了。
![](/img/in-post/200608_RNN/368b366734b1d1870f93760d9a2ad8a3.png)
4个输入，都是标量，3个为门电路1/0，1个输出
输入：1 2 3 4
![](/img/in-post/200608_RNN/5d87a536c27014bea93682939a274b48.png)
![](/img/in-post/200608_RNN/5a54cbc81f09545122d32c2663238f5b.png)
![](/img/in-post/200608_RNN/9f8221ed8e7d669d78759cb0715abd7f.png)
![](/img/in-post/200608_RNN/5f61233538816778a857f4c3c99cf335.png)
# 3 LSTM 对比传统rnn的好处
![](/img/in-post/200608_RNN/642a64294d986830e6e063aa70798253.png)
Can deal with gradient vanishing (not gradient explode)
**1.01的1000次方等于2w，因为rnn是对t时刻的循环加乘（量的加，权重的乘）容易出现梯度爆炸，**
**Lstm有了门电路后可以防止梯度爆炸；**
