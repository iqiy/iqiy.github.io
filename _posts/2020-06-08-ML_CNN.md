---
layout: post
title: "DL学习笔记（二）"
subtitle: "CNN基础"
date: 2020-06-08 22:20:18
author: "QIY"
header-img: "img/DL.png"
header-mask: 0.3
catalog: true
tags:
    - Deep Learning
---

> Deep Learning 的学习需稳扎稳打 积累自己的小厂库

* TOC

{:toc}

![](media/d9259be829b1cdb3d98a399ebc56defa.jpg)

# 一 CONV

a. 深度depth：神经元个数，决定输出的depth厚度。同时代表滤波器个数。

b. 步长stride：决定滑动多少步可以到边缘。

c.
填充值zero-padding：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。

![](media/46870a45bd1628814c60b8fbe055ace5.png)

如下：

两个神经元，即depth=2，意味着有两个滤波器。

数据窗口每次移动两个步长取3\*3的局部数据，即stride=2。

zero-padding=1。

![](media/aa3729e2ea758b1d80f602ecebd1d59c.png)

计算方法

![](media/ec5342395d32141f4657a47637c2ab94.png)

![](media/3eb40696b88f9fe020d6580d2e69fbcf.png)

+

![](media/1be4c3c6b4a189b770045573302cf387.png)

![](media/64218b5e6aba127598718bad82ffccda.png)

+

![](media/cfcf7e941e2295a9e10a8c303c069a0a.png)

![](media/3dfad5e133229a9be7745919d0243419.png)

\+1

= 1\* 0 + 1\*0 + -1\*0+ -1\*0 + 0\*0 + **1\*1**+ -1\*0 + -1\*0 + 0\*1+

\-1\*0 + 0\*0 + -1\*0+0\*0 + 0\*1 + **-1\*1**+1\*0 + -1\*0 + 0\*2+

0\*0 + 1\*0 + 0\*0+1\*0 + 0\*2 + 1\*0+0\*0 + -1\*0 + 1\*0

\+1 （偏置）= **1** ；

# 二 RELU

际梯度下降中，sigmoid容易饱和、造成终止梯度传递，且没有0中心化。咋办呢，可以尝试另外一个激活函数：ReLU，其图形表示如下

![](media/b6cecf94f3ecbb1a56b8455d545eea23.jpg)

ReLU的优点是收敛快，求梯度简单

# 三 POOL

池化，简言之，即取区域平均或最大，如下图所示（图引自cs231n）

![](media/d2489fe073395b5d8ce2c9b1ffec11b5.png)

# 四 FC

全连接层相当于做特征加权。最后的全连接层在整个卷积神经网络中起到 “分类器”
的作用。
