---
layout:     post
title:      "PRML学习笔记（六）"
subtitle:   "第六章 核方法"
date:       2018-11-08 00:15:18
author:     "Pelhans"
header-img: "img/prml.jpg"
header-mask: 0.3 
catalog:    true
tags:
    - PRML
---


> PRML 和 ESL 的学习基本上是学十得一。稳扎稳打再来一次

* TOC
{:toc}

# 核方法

许多线性模型可以被转化为一个等价的“对偶表示”。对偶表示中，预测的基础也是在训练数据点处计算的核函数(Kernel function) 的线性组合。对于基于固定特性非线性特征空间映射$\phi(x)$的模型来说，核函数由下面的关系给出：

$$ k(x, x^{'}) = \phi(x)^{T}\phi(x^{'}) $$

核替换方法可以使我们对许多著名的算法进行有趣的扩展，一般的思想是：如果我们有一个算法，它的输入向量x只以标量积的形式出现，那么我们可以用一些其他的核来替换这个标量积。

常用的核函数有各种不同的形式，其中许多和函数只是参数的差值的函数，即 $k(x, x^{'}) = k(x - x^{'}) $，这被称为静止核，因为核函数对输入空间的平移具有不变性。另一个核函数是同质核，也被称为径向基函数，它只依赖于参数之间的距离(通常是欧几里得距离)的大小，即 
$k(x, x^{'}) = k(||x, x^{'}||) $。

## 6.1 对偶表示 

考虑一个线性模型，它的参数通过最小正则化的平方和误差函数来确定。正则化的平方和误差函数为：

$$ J(w) = \frac{1}{2}\sum_{n=1}^{N}\{ w^{T}\phi(x_{n}) - t_{n} \}^{2} + \frac{\lambda}{2}w^{T}w $$

令 $J(W)$ 关于w的梯度为令，可以得到w的解：

$$ w = \Phi^{T}a $$

$$ a_{n} = -\frac{1}{\lambda}\{ w^{T}\phi(x_{n}) - t_{n} \} $$

其中 $\Phi$ 是设计矩阵，第n行为 $\phi(x_{n})^{T}$。使用a重新整理J 可以得到一个对偶表示：

$$ J(a) = \frac{1}{2}a^{T}\mathbf{KK}a - a^{T}\mathbf{Kt} + \frac{1}{2}\mathbf{t^{T}t} + \frac{\lambda}{2}a^{T}\mathbf{K}a $$

其中 $ \mathbf{K} = \mathbf{\Phi\Phi^{T}} $ 是 Gram矩阵，元素为：

$$ K_{nm} = \phi(x_{n})^{T}\phi(x_{m}) = k(x_{n}, x_{m}) $$

其中 $ k(x_{n}, x_{m}) $ 是核函数。由新的J表示，可以得到a的解：

$$ a = (K + \lambda\mathbf{I}_{N})^{-1}\mathbf{t} $$

因此，对于新输入的x，我们得到下面的预测：

$$ y(x) = k(x)^{T}(K + \lambda\mathbf{I}_{N})^{-1}\mathbf{t} $$

$$ k_{n}(x) = k(x_{n}, x) $$

因此我们看到对偶公式使得最小平房的问题的解完全通过核函数$k(x, x^{'})$表示。这被称为对偶公式，因为a的解可以被表示为$\phi(x)$的线性组合，从而我们可以视同参数向量w恢复出原始的公式。

对偶公式的优点是，它可以完全通过核函数来表示。于是我们可以直接针对很函数进行计算，避免了显示地引入特征向量$\phi(x)$，这使得我们可以隐式地使用高维特征空间，甚至无限维特征空间。

## 6.2 构造核

一种方法是选择一个特征空间映射 $\phi(x)$，然后使用这个映射寻找对应的核。另一种方法是直接构造核函数，但必须确保核函数是合法的。核函数 $k(x. x^{'})$是一个合法的核函数的充分必要条件是 Gram矩阵在所有的集合 $x_{n}$的选择下都是半正定的。除此之外我们还可以从一个概率生成模型开始构造核函数，这使得我们可以再一个判别式的框架中使用生成式模型。

给定合法的和$k_{1}(x, x^{'}) $和 $k_{2}(x, x^{'})$，则下面的新核也是合法的：

$$ \left\{
    \begin{aligned}
    k(x, x^{'}) & = & ck_{1}(x, x^{'}) \\
    k(x, x^{'}) & = & f(x)k_{1}(x, x^{'})f(x^{'}) \\
    k(x, x^{'}) & = & q(k_{1}(x, x^{'})) \\
    k(x, x^{'}) & = & exp(k_{1}(x, x^{'})) \\
    k(x, x^{'}) & = & k_{1}(x, x^{'}) + k_{2}(x, x^{'})\\
    k(x, x^{'}) & = & k_{1}(x, x^{'})k_{2}(x, x^{'}) \\
    k(x, x^{'}) & = & k_{3}(\phi(x), \phi(x^{'}))\\
    k(x, x^{'}) & = & x^{T}Ax^{'} \\
    k(x, x^{'}) & = & k_{a}(x_{a}, x_{a}^{'}) + k_{b}(x_{b}, x_{b}^{'}) \\
    k(x, x^{'}) & = & k_{a}(x_{a}, x_{a}^{'})k_{b}(x_{b}, x_{b}^{'})
    \end{aligned}
    \right.
$$


根据上面的性质，我们可以构造更复杂的核函数。这里我们重点提一下高斯核：

$$ k(x, x^{'}) = exp\left( -\frac{||x - x^{'}||}{w\sigma^{2}}  \right) $$

## 6.3 径向基函数网络

径向基函数中，每一个基函数只依赖于样本和中心$\mu_{j}$之间的径向距离(通常是欧几里得距离)，即：
$\phi(j) = h(||x-\mu_{j}||)$。选择基函数中心的一种最简单的方法是使用数据点的一个随机选择的子集。一个更加系统化的方法被称为正交最小平方。它是一个顺序选择的过程，在每一个步骤中，被选择作为基函数的下一个数据点对应于能够最大程度减小平方和误差的数据点。还可以使用聚类的方法，这时得到的基函数中心不再与训练数据点重合。

### 6.3.1 Nadaraya-Waston 模型

我们现在从核密度估计开始，以一个不同的角度研究核回归模型。假设我们有一个训练集$ \{x_{n}, t_{n}\}$，我们使用Parzen密度估计来对联合分布 $p(x, t)$ 建模，即：

$$ p(x, t) = \frac{1}{N}f(x - x_{n}, t - t_{n}) $$

其中 $f(x, t)$ 是分量密度，每个数据点都有一个以数据点为中心的这种分量。我们现在要找到回归函数f(x)的表达式，对应于以输入变量为条件的目标变量的条件均值：

$$ y(x) = E[t | x] = \frac{\sum_{n}\int t f(x - x_{n}, t - t_{n}) dt} { \sum_{m}\int t f(x - x_{m}, t - t_{m}) dt } $$

为了简化它，我们假设分量的密度函数的均值为零，即：

$$ \int_{-\infty}^{\infty} f(x, t)t dt = 0 $$

对所有的x都成立，使用一个简单的替换，有：

$$ y(x) = k(x, x_{n}) = \frac{g(x - x_{n})t_{n}}{\sum_{m}g(x - x_{m})} $$

$$ g(x) = \int_{-\infty}^{\infty} f(x, t) dt $$

上式y(x)被称为 Nadaraya-Waston 模型或 核回归、对于一个局部的函数，给距离x较近的数据点$x_{n}$较高的权重。

基于该模型的明显推广是使用高斯分布作为其分量，或者更一般地使用混合高斯分布对联合分布建模。

## 6.4 高斯过程

**高斯过程是观测值出现在连续域的统计模型。在高斯过程中，连续输入空间的每个点都是与一个正态分布的随机变量相关联。此外，这些随机变量的每个有限集合都有一个多元正态分布。高斯过程的分布是所有那些(无限多个)随机变量的联合分布。高斯过程观点的一个优点是，我们可以处理那些只能通过无穷多的基函数表达的协方差函数**

### 6.4.1 重新考虑线性回归问题

现在我们考虑线性回归问题，通过与函数$y(x, w)$的计算，重新推导出预测分布，这会给出高斯过程的一个具体的例子。

首先我们给出y(x)和先验分布p(w)的形式：

$$ y(x) = w^{T}\phi(x) $$

$$ p(w) = N(w | 0, \alpha^{-1}\mathbf{I}) $$

在实际应用中，我们希望计算这个函数在某个具体的x处的函数值。我们把函数值的集合记做向量$\mathbf{y}$，则：

$$ \mathbf{y} = \Phi w $$

首先，我们注意到$\mathbf{y}$是由w的元素给出的服从高斯分布的变量的线性组合，因此它本身是服从高斯分布的，于是我们只需要找到它的均值和方差即可。经计算：

$$ E[\mathbf{y}] = \Phi E[w] = 0 $$

$$ cov[\mathbf{y}] = E[\mathbf{y}\mathbf{y}^{T}] = \mathbf{K} $$

其中 K 是 Gram 矩阵，元素为：

$$ K_{nm} = k(x_{n}, x_{m}) = \frac{1}{\alpha}\phi(x_{n})^{T}\phi(x_{m}) $$

这个模型给出了高斯过程的一个具体的例子。通常来说，**高斯过程被定义为函数y(x)上的一个概率分布，使得在任意点集 $x_{1}, \dots, x_{N}$处计算的y(x)的值的集合联合起来服从高斯分布。在输入变量是二维的情况下，这被称为高斯随机场。更一般地，可以用一个合理的方式为$y(x_{1}), \dots, y(x_{N})$ 赋予一个联合概率分布，来确定一个随机过程y(x)**。

高斯随机过程的一个关键点是N个变量$y(x_{1}), \dots, y_{x_{N}}$ 上的联合概率分布完全由二阶统计来确定(即均值和方差)。

### 6.4.2 用于回归的高斯过程

为了把高斯过程模型应用于回归问题，我们需要考虑目标值的噪声，形式为：

$$ t_{n} = y_{n} + \epsilon_{n} $$

现在我们要考虑服从高斯分布的噪声过程，即：

$$ p(t_{n} | y_{n}) = N(t_{n}| y_{n}, \beta^{-1}) $$

$$ p(\mathbf{t} | \mathbf{y}) = N(\mathbf{t}| \mathbf{y}, \beta^{-1}\mathbf{I}_{N}) $$

根据高斯过程的定义，边缘概率分布p(y)是一个高斯分布，均值为零，协方差由 Gram矩阵 K 定义，即：

$$ p(\mathbf{y}) = N(\mathbf{y} | 0, \mathbf{K}) $$

确定K的核函数通常被选择成能够表示下面的性质：对于相似的点$x_{n}$和$x_{m}$，对应的值$y(x_{n})$和$y(x_{m})$的相关性要大于不相似的点。

为了求边缘概率分布，我们要对y积分，得：

$$ p(\mathbf{t}) = \int p(\mathbf{t} | \mathbf{y})p(\mathbf{y}) d\mathbf{y} = N(\mathbf{t} | 0, \mathbf{C}) $$

其中协方差矩阵C的元素为：

$$ C(x_{n}, x_{m}) = k(x_{n}, x_{m}) + \beta^{-1}\delta_{nm} $$

这个结果反映了两个事实：两个随机的高斯分布是独立的(即与y(x)有关的高斯和$\epsilon$有关的高斯)，因此他们可以简单的相加。

对于高斯过程回归，一个广泛使用的核函数的形式为指数项的二次型加上常数和线性项，即：

$$ k(x_{n}, x_{m}) = \theta_{0}exp\left\{ -\frac{\theta_{1}}{2}||x_{n} - x_{m}||^{2} \right\} + \theta_{2} + \theta_{3}x_{n}^{T}x_{m} $$

对于回归问题，我们的目标是计算
$p( t_{N+1} | t_{N} ) $。根据上面的计算可知，$p(t_{N+1})$是高斯分布，分布为：

$$ p(t_{N+1}) = N(t_{N+1} | 0, C_{N+1}) $$

因此条件概率分布
$p( t_{N+1} | t_{N} ) $ 也是高斯分布，均值和方差为：

$$ m(x_{N+1}) = K^{t}c_{N}^{-1}\mathbf{t} $$

$$ \sigma^{2}(x_{N+1}) = c - k^{T}C_{N}^{-1}\mathbf{k} $$

### 6.4.3 学习超参数

高斯过程模型的预测部分依赖于协方差函数的选择。在实际应用中，我们不固定协方差函数，而是更喜欢使用一组带有参数的函数，然后从数据中推断出参数的值。学习超参数的方法基于计算似然函数
$p(\mathbf{t} | \theta)$，其中$\theta$表示高斯过程模型的超参数。

### 6.4.5 用于分类的高斯过程

现在我们使用一个恰当的非线性激活函数，将高斯过程的输出进行变换来解决分类问题。

考虑一个二分类问题，给定目标变量$t\in {0,1}$。如果我们定义函数$a(x)$上的高斯过程，然后使用 sigmoid 函数 $y=\sigma(a)$进行变换，那么我们就得到了函数$y(x)$上的非高斯随机过程，其中$y\in(0,1)$。其中目标变量ｔ上的概率分布是伯努利分布：

$$ p(t|a) = \sigma(a)^{t}(1-\sigma(a))^{1-t} $$

我们的目标是确定预测分布
$ p(t_{N+1} | \mathbf{t}) $，为此我们引入向量$a_{N+1}$上的高斯过程先验：

$$ p(a_{N+1}) = N(a_{N+1} | 0, C_{N+1}) $$

由于数值计算的原因，我们引入一个由 参数$\nu$控制的类似噪声的项，它确保了协方差矩阵是正定的。因此协方差矩阵$C_{N+1} $的元素为：

$$ C(x_{n}, x_{m}) = k(x_{n}, x_{m}) + \nu\delta_{nm} $$

其中k是一个半正定的核函数，参数为$\theta$(通过最大似然优化得到)，$\nu$的值通常事先固定。因此：

$$ p(t_{N+1} = 1 | t_{N}) = \int p(t_{N+1} = 1 | a_{N+1})p(a_{N+1} | t_{N}) da_{N+1} $$

其中
$ p(t_{N+1} = 1 | a_{N+1}) = \sigma(a_{N+1})$。这个积分无法求出解析解，因此我们可以使用采样的方法来近似。除此之外，还可以通过对后验概率
$p(a_{N+1} | t_{N})$进行高斯近似。可以采用：基于变分推断、期望传播、拉普拉斯近似的方法去近似。

### 6.4.7 与神经网络的联系

在贝叶斯神经网络中，参数向量w上的先验分布以及网络函数$f(x,w)$产生了函数y(x)上的先验概率分布，其中y是网络输出向量。Neal已经证明，在极限 $M \rightarrow \infty$的情况下，对于w的一大类先验分布，神经网络产生的函数的分布将会趋于高斯过程。然而，应该注意，在这种极限情况下，神经网络的输出变量会变得相互独立。
