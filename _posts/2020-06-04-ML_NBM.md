---
layout:     post
title:      "ML学习笔记（三）"
subtitle:   "贝叶斯估计1"
date:       2020-06-04 21:20:18
author:     "QIY"
header-img: "img/ML.png"
header-mask: 0.3 
catalog:    true
tags:
    - Machine Learning
---


> Machine Learning 的学习需稳扎稳打

* TOC
{:toc}

# 1 背景

贝叶斯估计假设类条件概率密度p(x\|w)是一个多元正态分布，那么我们就可以把问题从估计完全未知的概率密度p(x\|w)转化成估计参数：如均值u、协方差ε；

KNN估计、Parzen窗这些就是非参数估计。

# 2 估计

所谓估计就是根据某条件估计概率，本文描述3个估计，其演化思路：MLE-\>MAP-\>贝叶斯估计。

![](/img/in-post/200604_NBM/c0cb1a8b6d669243c8973d9de3d756b3.png)

例子：硬币抛了10次，7正3反，设正面概率θ。

## 2.1 最大似然估计(maximum likelihood estimates，MLE)

即最大似然求θ；

回到抛硬币的例子，出现实验结果X的似然函数是什么呢？ 

$$
f(X,\theta) = \theta^{7}(1 - \theta)^{3}
$$

![](/img/in-post/200604_NBM/7351428011cc9a41f9c641caba6b3d36.png)

容易得出，在θ=0.7时，似然函数能取到最大值。

当然实际中我们一般不会画图，而是通过更为简洁的数学手段来处理。   
首先我们取对数似然函数，这样更方便后续的数学运算： 

$$
\ln(f(X,\theta)) = \ln(\theta^{7}(1 - \theta)^{3}) = 7\ln(\theta) + 3\ln(1 - \theta)
$$

对对数似然函数求导： 

$$
ln^{'}(f(X,\theta)) = \frac{7}{\theta} - \frac{3}{1 - \theta}
$$

令导数为0: 

$$
7(1 - \theta) - 3\theta = 0
$$

得：

$$
\theta = 0.7
$$

这样，我们已经完成了对   
的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7。是不是非常直接，非常简单粗暴？没错，就是这样，谁大像谁！ 

说到这里为止，可能很多同学不以为然：你这不坑爹嘛？只要硬币一枚正常硬币，不存在作弊情况，正面朝上的概率必然为0.5么，你这怎么就忽悠我们是0.7呢。OK，如果你这么想，恭喜你，那你就天然包含了贝叶斯学派的思想！我们所谓的正常硬币向上的概率为0.5，就是贝叶斯里的先验概率。

## 2.2 最大后验估计(maximum a posteriori estimation，MAP)

上面的最大似然估计MLE其实就是求一组能够使似然函数最大的参数，即

$$
{\overset{\hat{}}{\theta}}_{\text{ML}}(x) = arg\max_{\theta}f(x|\theta)
$$

如果这个参数θ有一个先验概率呢？比如上面的例子中，实际生活经验告诉我们，硬币一般都是均匀的，也就是θ=0.5的概率最大，那么这个参数该怎么估计？

这个时候就用到了我们的最大后验概率MAP。MAP的基础是贝叶斯公式：

$$
p(\theta|x) = \frac{p(x|\theta) \times p(\theta)}{P(x)}
$$

其中，p(x\|θ)就是之前讲的似然函数，p(θ)是先验概率，是指在没有任何实验数据的时候对参数
θ的经验判断，对于一个硬币，大概率认为他是正常的，正面的概率为0.5的可能性最大。

MAP优化的就是一个后验概率，即给定了观测值以后使后验概率最大：

$$
\begin{matrix}
{\overset{\hat{}}{\theta}}_{\text{MAP}} & = arg\max_{\theta}p(\theta|x) \\
 & = arg\max_{\theta}\frac{p(x|\theta) \times p(\theta)}{P(x)} \\
 & = arg\max_{\theta}p(x|\theta) \times p(\theta) \\
\end{matrix}
$$

从上面公式可以看出， p(x\|θ)是似然函数，而p(θ)就是先验概率。对其取对数：

$$
\begin{matrix}
arg\max_{\theta}p(x|\theta) \cdot p(\theta) & = arg\max_{\theta}\log\prod_{i = 0}^{n}p(x_{i}|\theta)p(\theta) \\
 & = arg\max_{\theta}\sum_{i}^{}\log(p(x_{i}|\theta)p(\theta)) \\
 & = arg\max_{\theta}\sum_{i}^{}\log(p(x_{i}|\theta) + \log(p(\theta)) \\
\end{matrix}
$$

通过导数为零方法 的θθ很可能接近于0.8；

## 2.3 贝叶斯估计

贝叶斯估计是在MAP上做进一步拓展，此时不直接估计参数的值，而是允许参数服从一定概率分布。回忆下贝叶斯公式：

$$
p(\theta|x) = \frac{p(x|\theta) \times p(\theta)}{P(x)}
$$

现在我们不要求后验概率最大，这个时候就需要求p(X)，即观察到的X的概率。一般来说，用全概率公式可以求p(X)

$$p(X) = \int p(X|\theta)p(\theta)\text{dθ}$$ 

那么如何用贝叶斯估计来预测呢？如果我们想求一个值 x’ 的概率，可以用下面的方法：

![](/img/in-post/200604_NBM/d9fb941cbbff92f7a59ee04f0e4e1eae.png)

解释如下：

![](/img/in-post/200604_NBM/c62815721137c784d92a114023b8b155.png)

特例：

当先验分布均匀之时，MAP 估计与 MLE
相等。直观讲，它表征了最有可能值的任何先验知识的匮乏。在这一情况中，所有权重分配到似然函数，因此当我们把先验与似然相乘，由此得到的后验极其类似于似然。因此，最大似然方法可被看作一种特殊的MAP。

如果先验认为这个硬币是概率是均匀分布的，被称为无信息先验( non-informative prior
)，通俗的说就是“让数据自己说话”，此时贝叶斯方法等同于频率方法。

随着数据的增加，先验的作用越来越弱，数据的作用越来越强，参数的分布会向着最大似然估计靠拢。而且可以证明，最大后验估计的结果是先验和最大似然估计的凸组合。

# 3 计算

在已知$$\theta$$情况下 求似然和后验：

假设有五个袋子，各袋中都有无限量的饼干(樱桃口味或柠檬口味)，已知五个袋子中两种口味的比例分别是

樱桃 100%

樱桃 75% + 柠檬 25%

樱桃 50% + 柠檬 50%

樱桃 25% + 柠檬 75%

柠檬 100%

问从同一个袋子中连续拿到2个柠檬饼干，那么这个袋子最有可能是上述五个的哪一个？

由于p的取值是一个离散值，即上面描述中的0,25%，50%，75%，1。我们只需要评估一下这五个值哪个值使得似然函数最大即可，得到为袋子5。这里便是最大似然估计的结果。

上述最大似然估计有一个问题，就是没有考虑到模型本身的概率分布，下面我们扩展这个饼干的问题。

假设拿到袋子1或5的机率都是0.1，拿到2或4的机率都是0.2，拿到3的机率是0.4，那同样上述问题的答案呢？这个时候就变MAP了。我们根据公式

写出我们的MAP函数。

根据题意的描述可知，p的取值分别为0,25%，50%，75%，1，g的取值分别为0.1，0.2,0.4,0.2,0.1.分别计算出MAP函数的结果为：0,0.0125,0.125,0.28125,0.1.由上可知，通过MAP估计可得结果是从第四个袋子中取得的最高。

参考：

<https://blog.csdn.net/liangjiubujiu/article/details/84871196>

<https://blog.csdn.net/bitcarmanlee/article/details/81417151>
