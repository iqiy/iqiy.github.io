---                                                                      
layout: post
title: "ML学习笔记（五）"
subtitle: "ML面试题总结"
date: 2020-09-06 14:20:18
author: "QIY"
header-img: "img/ML.png"
header-mask: 0.3
catalog: true
tags:
    - Machine Learning
---


> Machine Learning 的学习需稳扎稳打

* TOC
{:toc}

# 1 内外积


L题

# **1. 写出全概率公式&贝叶斯公式**

参考：

# **2. 模型训练为什么要引入偏差(bias)和方差(variance)？ 证**

偏差：指的是**模型预测值的期望**与真实值之间的差距。偏差越大，预测的值越偏离真实值。

方差：给定数据点的情况下，**模型预测的可变性，可以理解成模型预测的范围**。

![](/img/in-post/200906_ml/d015eb1ac8f5dab559120324a35ca80f.png)

偏差和方差的设置要有一定的权衡，高偏差低方差导致欠拟合，低偏差高方差导致过拟合；

# **3. CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型**

# **4. 如何解决过拟合问题？**

## 4.1 增加训练集

## 4.2  正则化（regularization）
损失函数分为经验风险损失函数和结构风险损失函数，结构风险损失函数就是经验损失函数+表示模型复杂度的正则化，正则项通常选择L1或者L2正则化。结构风险损失函数能够有效地防止过拟合。
L1正则化是指权值向量
![](/img/in-post/200906_ml/7d68a8c7a7794cb1d0dff2565b3de57b.gif)
中各个元素的绝对值之和，通常表示为
![](/img/in-post/200906_ml/f684e117282bf975882f4a7b4165db61.gif)
，L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择，一定程度上，L1也可以反之过拟合
L2化是指权值向量
![](/img/in-post/200906_ml/7d68a8c7a7794cb1d0dff2565b3de57b.gif)
中各个元素的平方和的平方，通常表示为
![](/img/in-post/200906_ml/c1ab0c0ee6b3a25e878821c445379e0d.gif)
，L2正则化可以防止模型过拟合
那L1和L2正则化是如何防止过拟合呢？首先我们先明白稀疏参数和更小参数的好处。
稀疏参数（L1）：参数的稀疏，在一定程度实现了特征的选择。稀疏矩阵指有很多元素为0，少数参数为非零值。一般而言，只有少部分特征对模型有贡献，大部分特征对模型没有贡献或者贡献很小，稀疏参数的引入，使得一些特征对应的参数是0，所以就可以剔除可以将那些没有用的特征，从而实现特征选择。
更小参数（L2）：越复杂的模型，越是尝试对所有样本进行拟合，那么就会造成在较小的区间中产生较大的波动，这个较大的波动反映出在这个区间内的导数就越大。只有越大的参数才可能产生较大的导数。试想一下，参数大的模型，数据只要偏移一点点，就会对结果造成很大的影响，但是如果参数比较小，数据的偏移对结果的影响力就不会有什么影响，那么模型也就能够适应不同的数据集，也就是泛化能力强，所以一定程度上避免过拟合。2.2
 正则化（regularization）
假设带有L1正则化的损失函数为：
![](/img/in-post/200906_ml/2435a8788337d5a846d84dd52fcc18e1.gif)
，当我们在
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
后添加L1正则化项时，相当于对
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
做了一个约束。此时我们的任务就变成在L1正则化约束下求出
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
取最小值的解。考虑二维的情况，在有两个权值
![](/img/in-post/200906_ml/84f88c8024f268286ce0a8884b5f67d2.gif)
和
![](/img/in-post/200906_ml/9b84221f0e179463506292051f930928.gif)
的情况下，此时L1为
![](/img/in-post/200906_ml/2087b989f386cc96fe33752738f31435.gif)
，对于梯度下降方法，求解
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
的过程用等值线表示，如下图所示。黑色方形是L1正则化的图形，五彩斑斓的等值线是
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
的等值线。在图中，
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
等值线与黑色方形首次相交的地方就是最优解。因为黑色方形棱角分明（二维情况下四个，多维情况下更多），
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
与这些棱角接触的几率要远大于其他部位接触的概率，而在这些棱角上，会有很多权值为0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。
![](/img/in-post/200906_ml/019f90836ee5966b87419c0f859fc5c8.png)
L1正则化
假设带有L2正则化的损失函数为：
![](/img/in-post/200906_ml/ef4aa8450673ffdc5d98ac5993025f52.gif)
，类似地，可以得到下图在二维平面上的图形。因为二维L2正则化函数是个圆，与L1的方形相比，圆滑了好多，因此
![](/img/in-post/200906_ml/02555c6263c26180c69e57711dec1b3a.gif)
和L2相交于棱角的几率比较小，而是更多权值取值更小。
![](/img/in-post/200906_ml/23d3b165e8dba9159049932b4e83c86f.png)
## 4.3 Dropout

在神经网络中，Dropout方法通过修改隐藏层神经元的个数来防止网络的过拟合，也就是通过修改深度网络本身。对于下图中左边的神经网络，在训练过程中按照给定的概率随机删除一些隐藏层的神经元，同时保证输入层和输出层的神经元不变。便能得到左边的神经网络，从而简化复杂的网络。

在每一批次数据被训练时，Dropout按照给定的概率P随机剔除一些神经元，只有没有被剔除也就是被保留下来的神经元的参数被更新。每一批次数据，由于随机性剔除神经元，使得网络具有一定的稀疏性，从而能减轻了不同特征之间的协同效应。而且由于每次被剔除的神经元不同，所以整个网络神经元的参数也只是部分被更新，消除减弱了神经元间的联合适应性，增强了神经网络的泛化能力和鲁棒性。Dropout只在训练时使用，作为一个超参数，然而在测试集时，并不能使用。

当前Dropout被广泛应用于全连接网络，而在卷积层，因为卷积层本身的稀疏性和ReLU激活函数的使用，Dropout在卷积隐藏层中使用较少

## 4.4  多任务学习

深度学习中两种多任务学习模式：隐层参数的硬共享和软共享

硬共享机制是指在所有任务中共享隐藏层，同时保留几个特定任务的输出层来实现。硬共享机制降低了过拟合的风险。多个任务同时学习，模型就越能捕捉到多个任务的同一表示，从而导致模型在原始任务上的过拟合风险越小。

软共享机制是指每个任务有自己的模型，自己的参数。模型参数之间的距离是正则化的，以便保障参数相似性。

参考：<https://blog.csdn.net/weixin_42111770/article/details/82703509>

# **5. One-hot的作用是什么？为什么不直接使用数字作为表示**

1）将类别变量转换为机器学习算法易于利用的一种形式的过程
。如：网络一般通过softmax层输出，它的输出是一个概率分布，从而要求输入的标签也以概率分布的形式出现，进而算交叉熵之类。

2）让特征之间的距离计算更加合理。如1，2，3对应的[1,0,0],[0,1,0],[0,0,1]之间距离都是sqrt(2)。否则直接用数字，13距离为2；12、23距离为1，凭什么呢？

原文：<https://blog.csdn.net/longshaonihaoa/article/details/107387559>

# **6. 决策树和随机森林的区别是什么？**

随机森林相对于决策树的优点主要是：

1）降低异常值所带来的影响：因为随机森林选取了部分数据建立了多个决策树，即使有个别决策树会因为异常值的影响导致预测不准确，但预测结果是参考多个决策树得到的结果，降低了异常值带来的影响。

2）降低了过拟合的可能性，因为决策树是采用了所有的特征及样本，容易出现过拟合（即对训练样本有很好的效果，对测试集的效果很差），随机森林是采用了部分样本的部分特征而构造的很多个决策树（采取的有放回抽样），特征和数据在单个决策树上变少了，降低了过拟合的可能性。

随机森林相对于决策树的缺点主要是：

1）计算量相对于决策树很大，性能开销很大。

2）可能会导致有些数据集没有训练到，但这种几率很小。

随机森林的优点：

1）可以处理高纬度的数据；

2）训练之前不需要特意的做特征选择；

3）建立很多树，预防了过拟合风险；

缺点：

计算量大，无法做到对实时数据的预测

# **7. 朴素贝叶斯为什么“朴素naive”？**

因为它假定所有的特征在数据集中的作用是同样重要和独立的，正如我们所知，这个假设在现实世界中是很不真实的，因此，说是很“朴素的”。

参考：

# **8. kmeans初始点除了随机选取之外的方法**

初始中心点的选择

初始中心点的选择最简单的做法是随机从样本中选K个作为中心点，但由于中心点的选择会影响KMeans的聚类效果，因此我们可以采取以下三种方式优化中心点的选取：

1.多次选取中心点进行多次试验，并用损失函数来评估效果，选择最优的一组；

2.选取距离尽量远的K个样本点作为中心点：随机选取第一个样本C1作为第一个中心点，遍历所有样本选取离C1最远的样本C2为第二个中心点，以此类推，选出K个初始中心点；

3.特别地，对于像文本这样的高维稀疏向量，我们可以选取K个两两正交的特征向量作为初始化中心点。

# **9. LR明明是分类模型为什么叫回归**

首先LR：Logistic
Regression，名称上看就是逻辑斯特回归，回归可理解为逐渐趋近，线性量，当对线性量分段去范围后就变成分类问题；

参考：https://www.cnblogs.com/Ph-one/p/13438339.html

# **10. 梯度下降如何并行化**

# **11. LR中的L1/L2正则项是啥**

L1正则是Laplace先验，L2是高斯先验。整个最优化问题可以看做是一个最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计

1. L2 regularizer ：使得模型的解偏向于 norm 较小的 W，通过限制 W 的 norm
的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge
regression 并不具有产生稀疏解的能力，得到的系数
仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。

2. L1 regularizer ： 它的优良性质是能产生稀疏性，导致 W 中许多项变成零。
稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。

# **12. 简述决策树构建过程**

# **13. 解释Gini系数**

# **14. 决策树的优缺点**

# **15. 出现估计概率值为 0 怎么处理**

# **16. 随机森林的生成过程**

# **17. 介绍一下Boosting的思想**

# **18. gbdt的中的tree是什么tree？有什么特征**

# **19. xgboost对比gbdt/boosting Tree有了哪些方向上的优化**

# **20. 什么叫最优超平面**

# **21. 什么是支持向量**

# **22. SVM如何解决多分类问题**

# **23. 核函数的作用是啥**
